{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ffe901",
   "metadata": {},
   "source": [
    "# Welcome to ESS-DIVE's Using Data with Dataset and Deep Dive API Jupyter Notebook\n",
    "This Jupyter Notebook is a workflow to help data users find and access ESS-DIVE datasets, particularly those that employ file-level metadata and csv reporting formats. The workflow includes: <br>\n",
    "**[Part 1: Searching for Data](#-Part-1-Searching-on-ESS-DIVE)**\n",
    "\n",
    "    Use the ESS-DIVE Dataset API and Deep Dive API to search for dataset files \n",
    "**[Part 2: Exploring Inside Datasets](#-Part-2-Exploring-Inside-Datasets)**\n",
    "    \n",
    "    Basic searching inside datasets\n",
    "    Use the Deep Dive API to explore within a dataset\n",
    "    Import data from csv files into python pandas dataframes\n",
    "**[Part 3: Starting Analysis](#-Part-3-Starting-Analysis)**\n",
    "    \n",
    "    Create simple visualizations with the data\n",
    "**[Part 4: Download Files and Log](#-Part-4-Download-Files-and-Save-the-Download-Log)**\n",
    "\n",
    "    Download files to local storage and log access details \n",
    "**[Part 5: Extra functionalities](#-Part-5-Extra-Resources-and-Examples)**    \n",
    "\n",
    "    Use the File-level Metadata (flmd) and Data Dictionaires (DD) to explore the dataset \n",
    "    Explore Sample Metadata to explore datasets with sample-based data [Section 6]\n",
    "\n",
    "This was created as a resource to the ESS-DIVE 2024 Community Data Workshop.\n",
    "\n",
    "Written By: Emily Nagamoto (she/her, LBNL), Danielle S Christianson (she/her, LBNL)\n",
    "\n",
    "Acknowledgements: This notebook builds from Danielle Christianson's [Finding and Accessing Data notebook](https://github.com/ess-dive/essdive-tutorials/blob/main/search_data/Tutorial_FindingAccessingData.ipynb), and Madison Burrus and Valerie Hendrix's Search & Download notebook.\n",
    "\n",
    "Last updated: 11/13/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc060b07",
   "metadata": {},
   "source": [
    "## README: How to use this notebook\n",
    "You will be running the cells in sequential order. The notebook is designed that you can just run every cell without changing anything, or you can enter your own inputs into cells marked with <strong><span style=\"color:blue\">Enter INPUT</span></strong>. If a cell is not marked with <strong><span style=\"color:blue\">Enter INPUT</span></strong> or is marked with <strong><span style=\"color:green\">Run Cell</span></strong>, then just run the cell without making changes. \n",
    "\n",
    "Optional view cells are marked with \"Optional\" in the first line. These do not need to be run, but are included for additional visualization or guidance.\n",
    "\n",
    "Any downloaded files are logged with the date/time of access. See Section 5 to save the log.\n",
    "\n",
    "Workflows:\n",
    "* Cells in **Part 1-4** are sequential and depend on variables entered in prior cells. \n",
    "* To use **Part 5**: *Section A* requires **Part 1-2**, *Section B* requires a different notebook - [Finding and Accessing Data notebook](https://github.com/ess-dive/essdive-tutorials/blob/main/search_data/Tutorial_FindingAccessingData.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1686044",
   "metadata": {},
   "source": [
    "# SET-UP - Run before any other cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485099d5-ffcb-46e3-98ff-183fe76c0791",
   "metadata": {},
   "source": [
    "### 1. Load packages that will be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62853ae7-a22a-4a73-bd29-5a28bc7d4a50",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7213b9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This notebook requires Python 3.\n",
    "import csv\n",
    "import datetime as dt\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "from urllib.request import Request, urlopen, urlretrieve\n",
    "from zipfile import ZipFile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc94dd4-71cf-48f2-903a-f68eb6dae678",
   "metadata": {},
   "source": [
    "### 2. Configure authentification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eaa558-bc0b-436e-8dad-16b4e2332177",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong> <br>\n",
    "1. Go to ESS-DIVE (https://data.ess-dive.lbl.gov/data), login with your ORCID, and copy your authentication token from your account settings page.\n",
    "2. Run the following code cell.\n",
    "3. Paste your authentication token into the prompt as requested. Hit `Enter` key.\n",
    "\n",
    "   _Always re-run this code cell when you update your token. Tokens expire every 24 hours._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = input('Token: ')\n",
    "\n",
    "essdive_api_url = 'https://api.ess-dive.lbl.gov'\n",
    "\n",
    "essdive_direct_url = 'https://data.ess-dive.lbl.gov/catalog/d1/mn/v2/object/'\n",
    "\n",
    "essdive_deepdive_url = 'https://fusion.ess-dive.lbl.gov'\n",
    "\n",
    "print('Success! Token is loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9950916",
   "metadata": {},
   "source": [
    "### 3. Configure local storage for downloads\n",
    "\n",
    "This cell will grab the current directory path as the path to save any downloads. The code is configured to create a new folder in the current directory to save any files there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377582e4-2012-46e9-b5f9-cd035fc21fe6",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72008418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new folder in current local directory\n",
    "new_dir = 'ESS-DIVE_Tutorial_Downloads'\n",
    "parent_dir = os.getcwd()\n",
    "download_dir_path = Path(os.path.join(parent_dir, new_dir))\n",
    "try: \n",
    "    os.mkdir(download_dir_path)\n",
    "    print(\"Directory '% s' created\" % new_dir)\n",
    "except:\n",
    "    print(\"This directory already exists.\")\n",
    "    \n",
    "if download_dir_path.exists():\n",
    "    print(f'Success! Local directory {download_dir_path} configured for downloads')\n",
    "    print('===================================')\n",
    "    current_files = [x for x in os.listdir(download_dir_path) if x != '.DS_Store']\n",
    "    if current_files:\n",
    "        print('Local directory contains: '+str(len(current_files)))\n",
    "    else:\n",
    "        print(f'Local directory is currently empty.')\n",
    "else:\n",
    "    print(f'Cannot find local directory {download_dir_path}. Please try again.')\n",
    "\n",
    "# create the file download log\n",
    "download_file_log = {}\n",
    "print('===================================')\n",
    "print('Downloaded files will be logged in the dictionary object \"download_file_log\".\\n'\n",
    "      'You can save this dictionary as a file later in the notebook.\\n'\n",
    "      'The filename, file url, and datetime accessed are recorded as a tuple in the \"downloaded_files\" element.')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33756161",
   "metadata": {},
   "source": [
    "### 4. Load general functions\n",
    "\n",
    "These are helper functions that we made to make printing information, creating pandas dataframes, and calling the API easier. Feel free to copy these functions to other notebooks as needed. Once you run the following cell, the functions can be used at any point in the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfc228-e32b-4815-8f29-c6bca79d1cf1",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc486f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_request(filename, f_url, stream=True):\n",
    "    \"\"\"\n",
    "    Get request for file, and stream the content back\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {'user_agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "               'content-type': 'application/json'}\n",
    "    try:\n",
    "        r = requests.get(f_url, headers=headers, verify=True, stream=stream)\n",
    "        status_code = r.status_code\n",
    "        if status_code == 200:\n",
    "            return r\n",
    "        else:\n",
    "            print(f\"{filename} request returned {status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"{filename} request unsuccessful: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def make_store(file_request, use_idx=True, print_headers=True):\n",
    "    \"\"\"\n",
    "    Read response and make store\n",
    "    \"\"\"\n",
    "    file_store = {}\n",
    "    csv_reader = csv.DictReader(file_request.iter_lines(decode_unicode=True))\n",
    "\n",
    "    for idx, row in enumerate(csv_reader):\n",
    "        if use_idx:\n",
    "            file_store.update({f'Index {idx}': row})\n",
    "            continue\n",
    "        fn = row.get('File_Name')\n",
    "        file_store.update({fn: row})\n",
    "    \n",
    "    headers = list(row.keys())\n",
    "    if print_headers:\n",
    "        print(f\"File headers: {headers}\")\n",
    "    return headers, file_store\n",
    "\n",
    "\n",
    "def inspect_dataset_distribution(dataset_detail, file_type='all'):\n",
    "\n",
    "    print(dataset_detail.get('name'))\n",
    "    print('========================================')\n",
    "\n",
    "    count = 0\n",
    "    dist = dataset_detail.get('distribution')\n",
    "    \n",
    "    for idx, file_info in enumerate(dist):\n",
    "        fn = file_info.get('name')\n",
    "        fn_url = file_info.get('contentUrl')\n",
    "        f_encoding = file_info.get('encodingFormat')\n",
    "        if file_type != 'all' and file_type not in f_encoding:\n",
    "            continue\n",
    "        print(f'Index {idx}: {fn}\\n  encoding: {f_encoding}\\n  url: {fn_url}')\n",
    "        count += 1\n",
    "        \n",
    "    if count == 0:\n",
    "        print(f'No files found that match the file_type: \"{file_type}\" criteria.')\n",
    "            \n",
    "            \n",
    "def retrieve_file_from_essdive(file_url, file_path):\n",
    "    \"\"\" Retrieve the data file \n",
    "        file_path includes file name.\n",
    "    \"\"\"     \n",
    "    error_messages = []\n",
    "    try:\n",
    "        urlretrieve(file_url, file_path)\n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        error_messages.append(f'Attempt 1 (no auth) failed: {e}')\n",
    "    try:\n",
    "        headers={\"Authorization\": f\"Bearer {token}\"}\n",
    "        request = urllib.request.Request(file_url, headers=headers)\n",
    "        \n",
    "        with urllib.request.urlopen(request) as response:\n",
    "            with open(file_path, 'wb') as out_file:\n",
    "                out_file.write(response.read())\n",
    "        return True, None\n",
    "    except urllib.error.HTTPError as e:\n",
    "        error_messages.append(f'Attempt 2 (with token) failed: HTTP Error {e.code}: {e.reason}')\n",
    "    except Exception as e:\n",
    "        error_messages.append(f'Attempt 2 (with token) failed: {str(e)}')\n",
    "        return False, ' | '.join(error_messages)\n",
    "    \n",
    "\n",
    "def download_selected_files(dataset_detail, file_indices, file_dir=download_dir_path, log_store=download_file_log, citation=None, \n",
    "                            is_csv_zipped=False, zip_download=None, zip_member_fn=None):\n",
    "    dist = dataset_detail.get('distribution')\n",
    "    ds_id = dataset_detail.get('@id')\n",
    "    #citation = dataset_detail.get('citation') << grabs related references but not the citation of the downloaded file\n",
    "    citation = citation\n",
    "    ds_name = dataset_detail.get('name')\n",
    "    \n",
    "    if log_store is None:\n",
    "        log_store = {}\n",
    "    \n",
    "    log_store.setdefault(ds_id, {'@id': ds_id, 'name': ds_name, 'citation': citation, 'downloaded_files': []})\n",
    "    ds_file_log = log_store.get(ds_id).get('downloaded_files')\n",
    "    \n",
    "    print(f'Saving files in {download_dir_path}')\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "    for idx, file_info in enumerate(dist):\n",
    "        msg = None\n",
    "        is_downloaded = None\n",
    "        \n",
    "        if idx not in file_indices:\n",
    "            continue\n",
    "            \n",
    "        fn = file_info.get('name')\n",
    "        file_path = download_dir_path / fn\n",
    "        fn_url = file_info.get('contentUrl')\n",
    "        \n",
    "        if not is_csv_zipped:\n",
    "    \n",
    "            download_ts = dt.datetime.now().isoformat()\n",
    "            is_downloaded, msg = retrieve_file_from_essdive(fn_url, file_path)\n",
    "    \n",
    "        else:\n",
    "            if not zip_download or not zip_member_fn:\n",
    "                print('ZipFile object and zipped member file name are required. Try again.')\n",
    "                return None\n",
    "            try:\n",
    "                zip_download.extract(zip_member_fn, path=file_path)\n",
    "                if Path.exists(file_path / zip_member_fn):\n",
    "                    is_downloaded = True\n",
    "                    download_ts = dt.datetime.now().isoformat()\n",
    "                else:\n",
    "                    msg = f'Extraction of {zip_member_fn} from {fn} was not successful.'\n",
    "            except Exception as e:\n",
    "                msg = f'ERROR attempting to extract {zip_member_fn} from {fn}: {e}'\n",
    "        \n",
    "        if is_downloaded:\n",
    "            print(f'--- {fn} downloaded')\n",
    "            ds_file_log.append((fn, fn_url, download_ts))\n",
    "        else:\n",
    "            print(msg)\n",
    "            \n",
    "    print(\"-------------------------------------\")\n",
    "    print(f'Remember to cite these files! Dataset DOI {ds_id}, \\nDataset citation: {citation}')\n",
    "    return ds_id    \n",
    "\n",
    "\n",
    "def inspect_zip_file_contents(dataset_detail, file_idx):\n",
    "    dist = dataset_detail.get('distribution')\n",
    "    file_info = dist[file_idx]\n",
    "    \n",
    "    if not file_info:\n",
    "        print('File index not found. Please try again.')\n",
    "        return\n",
    "    \n",
    "    fn = file_info.get('name')\n",
    "    if 'zip' not in file_info.get('encodingFormat'):\n",
    "        print(f'{fn} is not encoded as a zip file. Please select a different file.')\n",
    "    \n",
    "    fn_url = file_info.get('contentUrl')\n",
    "    resp = urlopen(fn_url)\n",
    "    \n",
    "    zip_download = ZipFile(io.BytesIO(resp.read()))\n",
    "    \n",
    "    print(f'{fn} contents:')\n",
    "    print('=================================')\n",
    "    for idx, file_member in enumerate(zip_download.namelist()):\n",
    "        print(f'Index {idx}: {file_member}')\n",
    "        \n",
    "    return fn, zip_download\n",
    "\n",
    "\n",
    "def read_zipped_csv(zip_file_obj, csv_file_name, header_rows=1):\n",
    "    # with open(zip_file_obj, mode='r') as z:\n",
    "    #     csv_df = pd.read_csv(io.BytesIO(z.read(csv_file_name)))\n",
    "    csv_df = pd.read_csv(zip_download.open(csv_file_name), skiprows=header_rows)\n",
    "    return csv_df\n",
    "\n",
    "\n",
    "def grab_metadata(r_json): # for fusiondb\n",
    "    df = pd.DataFrame()\n",
    "    records = []\n",
    "    \n",
    "    for dataset in r_json:\n",
    "        field_name = dataset['field_name']\n",
    "        unit = dataset['unit']\n",
    "        definition = dataset['definition']\n",
    "        data_type = dataset['data_type']\n",
    "        total_record_count = dataset['total_record_count']\n",
    "        values_summary = dataset['values_summary']\n",
    "        unit = dataset['unit']\n",
    "        doi = dataset['doi']\n",
    "        url = dataset['data_file_url']\n",
    "        data_file = dataset['data_file']\n",
    "        report={'Field_name':field_name, 'Unit':unit, 'Definition':definition, 'Data_type':data_type, \n",
    "                'Total_records':total_record_count,'Values':values_summary,'DOI':doi, \n",
    "                'URL':url,'File':data_file }\n",
    "        records.append(report) \n",
    "    \n",
    "    df = pd.DataFrame(records)  \n",
    "    return df\n",
    "\n",
    "# Change dataframe display options to better visualize the results\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.colheader_justify', 'left')\n",
    "    \n",
    "print('Functions loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca0947-3764-4a1a-9d1c-fa795702a9c3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff350720",
   "metadata": {},
   "source": [
    "# Part 1: Searching on ESS-DIVE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24a2b0-e525-41cf-8043-9b195e2c9801",
   "metadata": {},
   "source": [
    "## (A) Use the Dataset API tool\n",
    "Run this section to find datasets with the Dataset API tool. This section results in a list of potential datasets, and classification if it contains structured data or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc721ea",
   "metadata": {},
   "source": [
    "Use the ESS-DIVE Dataset API to search for datasets of interest.\n",
    "\n",
    "You can search for datasets using any of the following parameters:\n",
    "- Dataset Creator (**creator**): The creator/submitter of datasets\n",
    "- Date Published (**datePublished**): This is the date range of the publication of a package.\n",
    "- Project Name (**providerName**): The dataset project/provider that is set in the metadata.\n",
    "- Any text (**text**): Searches any metadata field that contains the passed text\n",
    "- Keywords (**keywords**): Search for datasets that have an exact match for all the given keywords.\n",
    "- Public datasets only (**isPublic**): If set with true, would only return public packages.\n",
    "\n",
    "**See additional details for dataset search in the ESS-DIVE package API techincal documentation:** https://api.ess-dive.lbl.gov/#/Data%20Package/listPackages.\n",
    "\n",
    "Use the [ESS-DIVE's project list](https://docs.google.com/spreadsheets/d/179SOyv42wXbP4owWZtUg3RqhW9dPOyENYcVYuUCcqwg/edit?usp=sharing) to find the options for project names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec89e6e-7697-473c-8fcb-fe8087b1e5be",
   "metadata": {},
   "source": [
    "### 1. Enter Search Parameters and make API call\n",
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c91b7-1435-46ee-8cbd-c37efddc99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter search terms: \"\\\"Leaf\"\\\" is an exact match, \"Leaf\" is any match\n",
    "creator=\"Forbes\"\n",
    "text= \"Yakima\"\n",
    "datePublished = \"[2020 TO 2024]\"  # \"<[YYYY TO YYYY-MM-DD]>\" # Not the same as data coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d2289-359d-4487-acae-0838553b3dd2",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct URL query to send to the ESS-DIVE packages API\n",
    "get_packages_response = f\"{essdive_api_url}/packages?creator={creator}&text={text}&datePublished={datePublished}&isPublic=true\"\n",
    "\n",
    "# Send request to API\n",
    "response = requests.get(get_packages_response, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "\n",
    "# Review the response and debug if needed\n",
    "if response.status_code == 200:\n",
    "    # Success\n",
    "    response_json = response.json()\n",
    "    print(\"Success! Continue to look at the search results\")  \n",
    "else:\n",
    "    # There was an error\n",
    "    print(\"There was an error. Stop here and debug the issue. Email ess-dive-support@lbl.gov if you need assistance. \\n\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b678f5",
   "metadata": {},
   "source": [
    "### 2. Inspect the search results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5358753-40ec-4ec5-91a5-e5d10a6ab890",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1ffee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here is a formatted version of what the response returns\n",
    "search_record_total = response_json['total']\n",
    "print(f\"Datasets found: {search_record_total}\")\n",
    "\n",
    "if search_record_total > 100:\n",
    "    print(\"The search API cannot return more than 100 results at a time. See documentation for how to paginate.\")\n",
    "\n",
    "candidate_datasets = response_json['result']\n",
    "\n",
    "for idx, dataset in enumerate(candidate_datasets):\n",
    "    print('-------------------')\n",
    "    print(f'Index: {idx}')\n",
    "    print(dataset.get('dataset').get('name'))\n",
    "    print(dataset.get('url'))\n",
    "    print(dataset.get('viewUrl'))\n",
    "    print(dataset.get('citation'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c2c08-a8f6-4a98-a508-cefbfa1326f5",
   "metadata": {},
   "source": [
    "#### ***Optional***: Want to see what the JSON response look like? Run the cell below. \n",
    "This cell will be available for most calls that we make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc3eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional: display entire response\n",
    "# ===================================\n",
    "display(response_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb3ec5",
   "metadata": {},
   "source": [
    "### 3. Subset search results - Which datasets do we want to explore further?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cba50-d0fa-4acd-a2e6-d1811c1230d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9b10b-b393-451f-aa9c-f5278dad72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick any that you are interested in\n",
    "record_indices = [1, 2, 4, 5, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd16dff-2231-4efe-8043-2707c56be5be",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c7dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = [candidate_datasets[x] for x in record_indices]\n",
    "citations_list = {}\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    print(f\"{idx}: {dataset.get('dataset').get('name')}\")\n",
    "    # grab the citations of the datasets to store for future use - Remember to always cite data sources you use!\n",
    "    citations_list.update({dataset.get('dataset').get('@id') : dataset.get('citation')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c59c8",
   "metadata": {},
   "source": [
    "### 4. Get dataset details using ESS-DIVE Dataset API\n",
    "\n",
    "Use the ESS-DIVE individual dataset search to get details of the datasets, including its list of files. The results of the above search contain the URLs to retrieve the dataset details in the field: `url`. \n",
    "\n",
    "The `get_dataset_details` method is a helper function that uses the same _requests.get_ from 'Step 1: Enter Search Parameters and make API call'.\n",
    "\n",
    "**See more details for the individual dataset search in the ESS-DIVE package API techincal documentation:** https://api.ess-dive.lbl.gov/#/Dataset/getDataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1addc15e-1771-4822-95d0-09f4578d8b40",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell - Helper Function</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e525d2a-37a7-4d7c-af3c-44339d22ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load this helper function that does the same GET call to the API but for specific files \n",
    "def get_dataset_details(dataset_url):\n",
    "    \n",
    "    response_status = None\n",
    "    try:\n",
    "        dataset_response = requests.get(dataset_url, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "        response_status = dataset_response.status_code\n",
    "    except Exception as e:\n",
    "        print(f\"{dataset.get('dataset').get('name')} did not have a successful return: {e}\")\n",
    "        return None\n",
    "\n",
    "    # If successful response, add to dataset_store\n",
    "    if response_status == 200:\n",
    "            dataset_json = dataset_response.json()['dataset'] \n",
    "            print(f\"--- Acquired details for {dataset_json.get('name')}\")\n",
    "            return dataset_json\n",
    "    elif response_status:  \n",
    "        print(f\"Response status {response_status}: {dataset_response.text}\")\n",
    "    else:\n",
    "        print(f\"Response status unavailable. Response cannot be interpreted. Debug required.\")\n",
    "    return None\n",
    "print('Function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea358f-7cae-46d8-bfc6-af87b6140061",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dataset details in a list\n",
    "dataset_details = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_url = dataset.get('url')\n",
    "    # see details for the get_dataset_details helper method in the cell above \n",
    "    dataset_detail_json = get_dataset_details(dataset_url)\n",
    "    if dataset_detail_json:\n",
    "        dataset_details.append(dataset_detail_json) \n",
    "\n",
    "print(\"=====================================\")\n",
    "print(f\"Details acquired for {len(dataset_details)} datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522da694",
   "metadata": {},
   "source": [
    "#### ***Optional***: Want to see what the dataset details look like? Select the input the number in the brackets for the index of the dataset you want to see and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded193a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional: Run to display dataset information for one of the datasets you chose - you can change number in the brackets to select\n",
    "# ===================================\n",
    "display(dataset_details[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2a3f5-a69a-44d7-95c1-6f7fd12a3b29",
   "metadata": {},
   "source": [
    "## (B) Use the Deep Dive API with the Fusion Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8aa6c2-15e2-429e-ac4e-0177470b04ef",
   "metadata": {},
   "source": [
    "The Fusion Database allows you to search within files and across datasets that follow structured data. Sometimes, datasets don't include all of the information in the metadata and thus may not come up in just the Dataset API search. You can search across all datasets available in the Fusion DB for specific field names. \n",
    "\n",
    "**See additional details for Deep Dive search in API techincal documentation:** https://fusion.ess-dive.lbl.gov/#/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7792c4c-f0a1-4018-8caa-b2cad7528ebf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Search within datasets for certain measured data\n",
    "The Fusion Database only searches structured data, meaning that the total list of potential datasets is limited. However, if you find datasets of interest, you will be able to explore inside them much more deeply. <br>\n",
    "You can search for datasets using any of the following parameters:\n",
    "- **rowStart** (integer, query): The row number to start on. Use this for paging results, minimum: 1\n",
    "- **pageSize** (integer, query): The number of datasets to return, maximum: 100\n",
    "- **doi** (string array, query): The digital object identifier (doi) representing a dataset\n",
    "- **fieldName** (string, query): The field name to search for, minLength: 1, maxLength: 100\n",
    "- **fieldDefinition** (string, query): Search the field definition, minLength: 1, maxLength: 100\n",
    "- **recordCountMin** (integer, query): Filter by record count greater that or equal to.\n",
    "- **recordCountMax** (integer, query): Filter by record count less than or equal to.\n",
    "- **fieldValueText** (string, query): Filter by a text field value. Search is case insensitive\n",
    "- **fieldValueNumeric** (integer, query): Filter by a numeric value that is between min and max summary values.\n",
    "- **fieldValueDate** (string($date), query): Filter by a date/datetime value that is between min and max summary values. Date format: (yyyy-mm-dd), Datetime format: (yyyy-mm-ddTHH:MM:SS)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99871d8-96b4-498d-9c22-555f27c2e09f",
   "metadata": {},
   "source": [
    "### General Search\n",
    "You can search within individual DOIs, multiple DOIs, or across all available datasets that are available in the Fusion Database. Here, we will do a search without specifying the specific DOI, to explore if there are other datasets of interest. In the next section, we will do searches on a couple of DOIs to see if they have specific files we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e431a30",
   "metadata": {},
   "source": [
    "### 1. Enter Search Parameters and make API call\n",
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46126330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter search terms\n",
    "# For an exact match, put the string in quotes, e.g. \"\\\"Leaf\"\\\" is an exact match, \"Leaf\" is any match\n",
    "fieldName=\"conductance\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b36e83",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f372ea-a3d4-47c0-9b81-8c532f5f02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct URL query to send to the Deep Dive API\n",
    "get_deepdive_response = f\"{essdive_deepdive_url}/api/v1/deepdive?rowStart=1&pageSize=100&fieldName={fieldName}\"\n",
    "\n",
    "# Send request to API\n",
    "response_deepdive = requests.get(get_deepdive_response)\n",
    "\n",
    "# Review the response and debug if needed\n",
    "if response_deepdive.status_code == 200:\n",
    "    # Success\n",
    "    response_json_deepdive = response_deepdive.json()\n",
    "    results_deepdive = response_deepdive.json()['results']\n",
    "    print(\"Success! Continue to look at the search results\")  \n",
    "else:\n",
    "    # There was an error\n",
    "    print(\"There was an error. Stop here and debug the issue. Email ess-dive-support@lbl.gov if you need assistance. \\n\")\n",
    "    print(response_deepdive.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58dafe-878a-4890-9b39-688c7aac533b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: View the JSON response\n",
    "# ===================================\n",
    "display(response_json_deepdive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027645f-8e74-425d-a28e-95f6629d7b4d",
   "metadata": {},
   "source": [
    "### 2. Inspect the search results - as a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a249e8-f847-40b5-86d7-804cf84b0d55",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63c45a-2bbc-4fe9-91ba-1dc842763e07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create and display a pandas dataframe for the report\n",
    "project_report_deepdive =grab_metadata(results_deepdive)\n",
    "display(project_report_deepdive.style.set_properties(**{'text-align': 'left'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2db726",
   "metadata": {},
   "source": [
    "### This example for \"conductance\" headers returns 98 files that match this search. How do we narrow the search down further? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee80b63",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed274af-d5f7-492d-934f-827d80857c09",
   "metadata": {},
   "source": [
    "# Part 2: Exploring Inside Datasets \n",
    "Let's look inside the datasets we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739c67e",
   "metadata": {},
   "source": [
    "### 1. Which datasets have File Level Metadata (FLMD)?\n",
    "Some datasets are structured with FLMDs and some are not. Depending on the file structure, we can approach further exploration differently. \n",
    "\n",
    "#### Here is a helper function `assess_datasets_flmd_dd_csv_files` that will inspect a list of datasets and search the files in a dataset for `flmd` files. It will return two lists of datasets - one for datasets that have a readily accessible FLMD (not in a zip file) and ones that do not (either no FLMD or it is in a zip file).\n",
    "The utility of this function allows us to get a sense of which tools may be the most helpful in determining if a dataset will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1263a41-a04c-4ad3-ae9c-2693d8060979",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell - Helper Function</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101a071-68b8-42fd-8890-dc0d15574440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_datasets_flmd_dd_csv_files(dataset_details_list):\n",
    "    \"\"\"\n",
    "    Find the datasets with flmd files\n",
    "    Sort the csv file contents into potential and data files; add to the dataset details dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    flmd_datasets_indices = set()\n",
    "    flmd_dataset_details = []\n",
    "    \n",
    "    for idx, dataset in enumerate(dataset_details_list):\n",
    "        file_list = dataset.get('distribution')\n",
    "    \n",
    "        flmd_url = {}\n",
    "        csv_files = {}\n",
    "        for f in file_list:\n",
    "            encoding_format = f.get('encodingFormat')\n",
    "            filename = f.get('name')\n",
    "            url = f.get('contentUrl')\n",
    "        \n",
    "            if 'csv' not in encoding_format or url is None:\n",
    "                continue\n",
    "        \n",
    "            if 'flmd' in filename:\n",
    "                flmd_datasets_indices.add(idx)\n",
    "                flmd_url.update({filename: url})\n",
    "        \n",
    "            else:\n",
    "                csv_files.update({filename: url})\n",
    "\n",
    "        dataset.update({\n",
    "            'flmd_url': flmd_url,\n",
    "            'csv_files': csv_files\n",
    "        })\n",
    "    \n",
    "        if not flmd_url:      \n",
    "            dataset_name = dataset.get('name')\n",
    "            print(f\"No flmd found for dataset: {dataset_name}\")\n",
    "        \n",
    "    print(\"=====================================\")\n",
    "    \n",
    "    if len(flmd_datasets_indices) > 0:\n",
    "        print(f'flmd found in {len(flmd_datasets_indices)} datasets')\n",
    "        flmd_dataset_details = [dataset_details_list[x] for x in flmd_datasets_indices]\n",
    "    else:\n",
    "        print(f'No datasets in the search results have flmds.')\n",
    "        \n",
    "    no_flmd_dataset_details = [dataset_detail for idx, dataset_detail in enumerate(dataset_details_list) if idx not in flmd_datasets_indices]\n",
    "    \n",
    "    return flmd_dataset_details, no_flmd_dataset_details\n",
    "print('Function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d127156c-5a7f-4c82-9b6d-3ee8b04708e3",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27036b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the helper function assess_datasets_flmd_dd_csv_files to determine which files have readily accessible flmd\n",
    "flmd_datasets, no_flmd_datasets = assess_datasets_flmd_dd_csv_files(dataset_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadce836-1abb-46fd-bac1-bb85cedc3bfd",
   "metadata": {},
   "source": [
    "## A. Basic Search: Manually look inside the datasets\n",
    "_Inspect dataset using Dataset Details Distribution_ <br>\n",
    "Useful for a preliminary search into files without readily accessible FLMDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28674de2",
   "metadata": {},
   "source": [
    "### 1. List the datasets that do not have flmd files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5a3cc-fed9-42bc-85fc-9bde909c9caf",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the datasets we just identified \n",
    "for idx, fd in enumerate(no_flmd_datasets):\n",
    "    print(f\"--- Index {idx}: {fd.get('name')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a3e547",
   "metadata": {},
   "source": [
    "### 2. Choose dataset to inspect using index above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d1254-c22a-4b00-a0e3-f7ab1f847360",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a4756-3b84-473c-b346-2640c5630235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset you want to look at and decide which files you want to print out\n",
    "ds_idx_no_flmd = 1\n",
    "file_type = 'all'  # 'all' or 'csv' or 'pdf' or 'zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207e8ce-4e09-481c-a673-7df51eb74422",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9872076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use this helper function to print the names of the files in the dataset you chose\n",
    "inspect_dataset_distribution(no_flmd_datasets[ds_idx_no_flmd], file_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e43e1-f7e9-43b4-9f49-f27f029dc50e",
   "metadata": {},
   "source": [
    "### 3. Select zip file to inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c1a6d-4638-4c38-9c9b-ab697db6555a",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afd2ab-7f98-45ce-9bc7-1284fc48db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the specific dataset details from the dataset we chose from the list of datasets we selected originally:\n",
    "dataset_detail = dataset_details[1]\n",
    "\n",
    "# Index of zip file from file distribution\n",
    "zip_file_index = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775be58b-024d-4d62-bb6f-4e0bb010d954",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26ebfa-6c4f-4bbc-afc4-2e6f6da63545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use this helper function to list the files in the zip file \n",
    "fn, zip_download = inspect_zip_file_contents(dataset_detail, zip_file_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d097d-d43d-48b2-86ca-74d6e20d2817",
   "metadata": {},
   "source": [
    "### 4. Select csv file within zip file that you want to inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96203c52-d1c2-441f-bc9b-ab4db42ed5bb",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f6ffb-06f2-4d36-a78d-9f6908eb2227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the index for the file you want to look at\n",
    "csv_file_idx = 170"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e0da9-a864-4d7e-bc1d-3036c7f68291",
   "metadata": {},
   "source": [
    "#### Before you can view the file, let's take a look at the file structure to understand how to parse it.\n",
    "For this tutorial, we know this dataset has structured CSV files and it may have multiple rows of metadata. Let's look at the first line to see where the header rows start.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540537cb-1c2a-4f91-8c33-64bc671f7ec5",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8b861-cff0-4506-a78d-522a0f82bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first line of the file and extract header row number\n",
    "# ===================================\n",
    "csv_file_name = zip_download.namelist()[csv_file_idx]\n",
    "\n",
    "header_row = 0\n",
    "with zip_download.open(csv_file_name) as f:\n",
    "    line = f.readline().decode('utf-8')  # Decode the bytes to string\n",
    "    print(line)\n",
    "    if \"# HeaderRows_\" in line:\n",
    "        header_row = int(line.split(\"# HeaderRows_\")[1])  # Extract the number part\n",
    "        print(f\"Extracted header row number: {header_row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e7048-2118-4f00-aaec-e602c432299f",
   "metadata": {},
   "source": [
    "#### This CSV happens to follow the CSV Guidelines and we can easily print out the number of header rows. To verify that this is true, we'll print this number of rows first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa64c95-789f-4e92-b054-19f0c5963d14",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d2604-c404-41dd-8abd-08879a46a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out rows up to header row number\n",
    "if header_row > 0:\n",
    "    with zip_download.open(csv_file_name) as f:\n",
    "        for i in range(header_row):\n",
    "            print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ac871-f446-4e06-a891-2ed898bbf6b4",
   "metadata": {},
   "source": [
    "#### Look at the last line that is printed - that should be the column names!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d5700c-5689-4bd6-ba7c-bcf6a956ae8f",
   "metadata": {},
   "source": [
    "#### So to correctly put a csv file into a pandas dataframe, you want to take that header row number (7 in this example) and subtract 1, to keep the row with the data column names. In this example we want to skip 6 rows.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ec3c3-93af-47f5-aef5-769842f8c2a0",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd03d5-eaa2-47e7-b75f-794177e602cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows_to_skip = header_row - 1\n",
    "print(f'The header row is row {header_row}, so we will skip {rows_to_skip} rows of the file')\n",
    "\n",
    "print(f'Attempting to read: {csv_file_name} from zip file {fn}')\n",
    "\n",
    "metadata_df = read_zipped_csv(zip_download, csv_file_name, rows_to_skip)\n",
    "zip_download_1_datasetapi = zip_download\n",
    "fn_datasetapi = fn\n",
    "csv_file_name_datasetapi = csv_file_name\n",
    "\n",
    "if metadata_df is not None:\n",
    "    is_csv_zipped = True\n",
    "    headers = list(metadata_df.columns)\n",
    "    data_df_datasetapi = metadata_df\n",
    "    display(metadata_df)\n",
    "else:\n",
    "    print('ERROR: Sample metadata file was not successfully loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494fa9b6-bf40-4fd7-a4d4-3135669b5800",
   "metadata": {},
   "source": [
    "## B. Advanced Search: Use API tools to look inside data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c650b-5a0d-48ee-9724-12635ff0ce40",
   "metadata": {},
   "source": [
    "_Inspect datasets with structured data (FLMD)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35478109-e4ef-4dfa-bfaa-b864ce2f3163",
   "metadata": {},
   "source": [
    "### 1. Use the Deep Dive API (Query-Data) to look in specific datasets\n",
    "Using the datasets that **do** have FLMD, we will explore inside these files to find ones we are interested in for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5152d-e52a-4fa7-964e-7854dd06fd6e",
   "metadata": {},
   "source": [
    "In Part 1 we used the Deep Dive (Query-Data) to look for files with certain terms across pany public dataset that is in the Fusion DB.\n",
    "\n",
    "Now, we will specify which datasets we want to look at to see (a) if they are available on Deep Dive and (b) what specific files may be of interest. We will need their DOIs to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21f0da-19cc-470e-a39e-1651d9741901",
   "metadata": {},
   "source": [
    "***The DOIs that we will use in this example come from our Dataset API search results (Part 1, Section A, Step 3: Subset Search Results).***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc22230-1531-4d6a-8186-1d280c63b271",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536344ac-e09c-43b3-8725-71a80bcbf3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the DOIs for our selected datasets\n",
    "\n",
    "total_doi_array = []\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    print(f\"{idx}: {dataset.get('dataset').get('@id')}, {dataset.get('dataset').get('name')[:25]}...\")\n",
    "    total_doi_array.append(dataset.get('dataset').get('@id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331aeb5e-6d99-4788-8c2e-4f999714c909",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75295542-9157-4be4-b6a9-47f72ebbd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter search terms\n",
    "fieldName=\"conductance\"\n",
    "\n",
    "# Select the datasets that you would like to check. \n",
    "# Change the indices in the bracket for the indices of the datasets from the cell above \n",
    "doi_array = total_doi_array[0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e3392-e8c7-47d3-8e91-cf5bb93519e1",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3412bf4-352f-4ce9-bb56-97eb14f1bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct URL query to send to the Deep Dive API\n",
    "doi_information=\"\"\n",
    "for d in doi_array:\n",
    "    doi_information=doi_information + \"&doi=\"+d\n",
    "\n",
    "get_deepdive_response = f\"{essdive_deepdive_url}/api/v1/deepdive?rowStart=1&pageSize=100&fieldName={fieldName}{doi_information}\"\n",
    "\n",
    "# Send request to API\n",
    "response_deep_dive = requests.get(get_deepdive_response)\n",
    "\n",
    "# Review the response and debug if needed\n",
    "if response_deep_dive.status_code == 200:\n",
    "    # Success\n",
    "    response_json_deep_dive = response_deep_dive.json()\n",
    "    results_deep_dive = response_deep_dive.json()['results']\n",
    "    print(\"Success! Continue to look at the search results\")  \n",
    "else:\n",
    "    # There was an error\n",
    "    print(\"There was an error. Stop here and debug the issue. Email ess-dive-support@lbl.gov if you need assistance. \\n\")\n",
    "    print(response_deep_dive.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243817ba-dcac-4438-b1e4-a5246ca38135",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f1e67a-791e-47ea-96d2-fd9f66c9ddf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: View the JSON response\n",
    "# ===================================\n",
    "display(response_json_deep_dive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ab288",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### In this example, I'm interested in looking at the results with the most amount of data records. I sorted my table to show me which those are so I can easily reference the index.\n",
    "There is also the option to the view the table unsorted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a608b1-76f5-4844-b1ed-8735d053c62a",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4fcb6-7cda-4d04-8cd6-02ac63e524a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create pandas dataframe for the report\n",
    "project_report_deep_dive = grab_metadata(results_deep_dive)\n",
    "\n",
    "# This code sorts the dataframe by total records\n",
    "columns_to_sort = ['Total_records']\n",
    "ascending = [False]\n",
    "project_report_sorted_deep_dive = project_report_deep_dive.sort_values(by=columns_to_sort,ascending=ascending)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb278e-1ed8-4729-bcf3-6a229207a5b4",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acf4a8-b487-43b2-8b37-16ede2069948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Choose the dataframe to display - Sorted or Non-Sorted\n",
    "## =================\n",
    "## Display Sorted dataframe\n",
    "display(project_report_sorted_deep_dive.style.set_properties(**{'text-align': 'left'}))\n",
    "\n",
    "## Uncomment to display Non-Sorted dataframe \n",
    "#display(project_report_deep_dive.style.set_properties(**{'text-align': 'left'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b2e63-0c8d-4826-8bb1-ace02a458456",
   "metadata": {},
   "source": [
    "### Let's grab the file(s) that we are interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a298d04",
   "metadata": {},
   "source": [
    "## 3. Use Get-Dataset-File to identify specific files\n",
    "Aside from identifying specific files in datasets, the Deep Dive API can also retrieves a dataset file by its file path, using a different request message (called an end point). <br>\n",
    "Learn more at Fusion docs: [Get-Dataset-File](https://fusion.ess-dive.lbl.gov/#/default/get_dataset_file_api_v1_deepdive__doi___file_path__get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde6bd0-0da7-4c1b-b42d-efcfcd700459",
   "metadata": {},
   "source": [
    "### From the previous list of files, we will use the index to then grab the DOI and file name to query the Deep Dive API. \n",
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86eab1-cc97-46b4-a964-298f84f6d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an index from the pandas dataframe to choose a file to investigate \n",
    "i_of_interest= 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e45c0c-cab4-4d53-9fd9-bf92e6b18b03",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf07123-6839-48c8-8ab6-7ebf9d543c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format for the URL Deep Dive call is - DOI:file_name\n",
    "doi_file_information = project_report_sorted_deep_dive.loc[i_of_interest]['DOI'] + ':' + project_report_sorted_deep_dive.loc[i_of_interest]['File']\n",
    "# using the DOI, grab the index from the flmd_datasets\n",
    "index_for_datasets = doi_array.index(project_report_sorted_deep_dive.loc[i_of_interest]['DOI']) \n",
    "\n",
    "# Contruct URL query to send to the Deep Dive API\n",
    "get_deepdive_response_file = f\"{essdive_deepdive_url}/api/v1/deepdive/{doi_file_information}\"\n",
    "\n",
    "# Send request to API\n",
    "response_deepdive_file = requests.get(get_deepdive_response_file)\n",
    "\n",
    "# Review the response and debug if needed\n",
    "if response_deepdive_file.status_code == 200:\n",
    "    # Success\n",
    "    response_deepdive_file_json = response_deepdive_file.json()\n",
    "    print(f\"Success for file {doi_file_information}! Continue to look at the search results\")  \n",
    "else:\n",
    "    # There was an error\n",
    "    print(\"There was an error. Stop here and debug the issue. Email ess-dive-support@lbl.gov if you need assistance. \\n\")\n",
    "    print(response_deepdive_file.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbe9ee-b111-4b99-9780-324a4adddb91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional: display entire json response\n",
    "# ===================================\n",
    "#display(response_deepdive_file_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd92b7",
   "metadata": {},
   "source": [
    "### Great! Now we have identified the file we want through the Deep Dive API. Next, we'll to look into the file itself, to see if we want to download the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc62c60-026f-4f6e-b802-6d5c1d189eed",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341bbd41-ebc5-4d29-a7e8-c9f69ab1c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We using the file information to grab it and then visualize it  \n",
    "current_response_json = response_deepdive_file_json\n",
    "\n",
    "fn_url = current_response_json['data_download']['contentUrl']\n",
    "\n",
    "headers={\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# Create a request with headers\n",
    "req = Request(fn_url, headers=headers)\n",
    "\n",
    "# Open the URL with the added headers\n",
    "try:\n",
    "    resp = urlopen(req)\n",
    "    zip_download = ZipFile(io.BytesIO(resp.read()))\n",
    "    print('Success!')\n",
    "    \n",
    "except urllib.error.HTTPError as e:\n",
    "    print(f'HTTPError: {e.code} - {e.reason}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb63701",
   "metadata": {},
   "source": [
    "#### We want to visualize this file in a pandas dataframe - thus we need to identify what the header row is. \n",
    "Let's find out by printing the first couple of lines of the file. The first line should contain a string like ` b'# HeaderRows_10\\n' `, and the number is the line of the file where the header row is.  <br>\n",
    "We will try this to identify the line where the header row is in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e32531f-730e-4a44-b910-913dd35eadc8",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be84f970-fb50-42f7-9472-c7d1fa8dbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first 2 lines of the file and extract header row number\n",
    "# ===================================\n",
    "csv_file_name_deep_dive = current_response_json['data_file']\n",
    "csv_file_name_deep_dive = csv_file_name_deep_dive.split('.zip/', 1)[1]\n",
    "header_row = 0\n",
    "with zip_download.open(csv_file_name_deep_dive) as f:\n",
    "    line = f.readline().decode('utf-8')  # Decode the bytes to string\n",
    "    print(line)\n",
    "    if \"# HeaderRows_\" in line:\n",
    "        header_row = int(line.split(\"# HeaderRows_\")[1])  # Extract the number part\n",
    "        print(f\"Extracted header row number: {header_row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1044c",
   "metadata": {},
   "source": [
    "#### You can then verify this by printing this number of lines to see if you get a row of header. Look at the last line that is printed - that should be the column names!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0df9407-5600-4e08-ae9c-44f1ad406e1d",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c98a1c3-367e-4675-99cd-de69a543afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out rows up to header row number\n",
    "if header_row > 0:\n",
    "    with zip_download.open(csv_file_name_deep_dive) as f:\n",
    "        for i in range(header_row):\n",
    "            print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4fcd8-95cc-4d6b-ba2a-457876683649",
   "metadata": {},
   "source": [
    "#### So to correctly put a csv file into a pandas dataframe, you want to take that header row number (7 in this example) and subtract 1, to keep the row with the data column names. In this example we want to skip 6 rows.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2db20b-fce0-4285-9938-23963b0d7399",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36dffec-851a-469d-b9db-dfca831ed293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows_to_skip = header_row - 1\n",
    "print(f'The header row is row {header_row}, so we will skip {rows_to_skip} rows of the file')\n",
    "\n",
    "fn = current_response_json['data_download']['name']\n",
    "print(f'Attempting to read: {csv_file_name_deep_dive} from zip file {fn}')\n",
    "\n",
    "metadata_df = read_zipped_csv(zip_download, csv_file_name_deep_dive, rows_to_skip)\n",
    "zip_download_2_deep_dive = zip_download\n",
    "fn_deep_dive = fn\n",
    "\n",
    "if metadata_df is not None:\n",
    "    is_csv_zipped = True\n",
    "    headers = list(metadata_df.columns)\n",
    "    data_df_deep_dive = metadata_df\n",
    "    display(metadata_df)\n",
    "else:\n",
    "    print('ERROR: Sample metadata file was not successfully loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da82c2",
   "metadata": {},
   "source": [
    "### Success! We have identified a number of files that could be relevant and we have opened one file for this example. Let's move on to visualizing this example file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91d33c",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d18d1-0fbb-4fca-bc88-d5d6caa8b683",
   "metadata": {},
   "source": [
    "## Alternate Approach to Viewing Files: \n",
    "### What to do if the file you want is not in Deep Dive? \n",
    "You can use a manual approach to looking at file level metadata and data dictionaries to find specific files to then view. This method is in the **[Part 5. Extra Resources and Examples](#-Part-5-Extra-Resources-and-Examples)** in **[Section A. Manual Inpsection](##-A-Manually-inspecting-the-FLMD-and-Data-Dictionary-(DD))**. Or you can refer back to **[Part 2: A. Basic Search](##A-Basic-Search-Manually-look-inside-thedatasets)** <br>\n",
    "**_NOTE:_ We are skipping this section for the 2024 Using Data Workshop Tutorial for the sake of time.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c391ba-9252-416b-9091-bf4c28764b68",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977689cf",
   "metadata": {},
   "source": [
    "# Part 3: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4b063-ee9a-4e7c-b396-8ce70d634f5a",
   "metadata": {},
   "source": [
    "## A. Begin Simple Analysis - using results from basic and advanced searches\n",
    "Now that we have identified files of interest, let's start using them and begin our investigation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0cc11",
   "metadata": {},
   "source": [
    "### 1. Load the two selected csv data files into pandas dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1a06a-6ec2-43bf-a7d1-71810ad25d0f",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b03a95-93ca-495d-985f-e7684c4799ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data identified from Basic Search\n",
    "# ===================================\n",
    "# grab and print identifying information from the dataset details\n",
    "index_dataset_api_dataset = total_doi_array.index(no_flmd_datasets[ds_idx_no_flmd].get('@id'))\n",
    "print(datasets[index_dataset_api_dataset].get('dataset').get('@id'))\n",
    "print(datasets[index_dataset_api_dataset].get('dataset').get('name'))\n",
    "data_df_datasetapi_name = datasets[index_dataset_api_dataset].get('dataset').get('name')\n",
    "print(datasets[index_dataset_api_dataset].get('viewUrl'))\n",
    "\n",
    "# display the pandas dataframe containing the datafile\n",
    "display(data_df_datasetapi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b56eb9-96ea-48e3-a51f-a815c5c02a4f",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10d590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data identified from Advanced Search\n",
    "# ===================================\n",
    "# grab and print identifying information from the dataset details\n",
    "index_deep_dive_dataset = total_doi_array.index(current_response_json.get('doi'))\n",
    "print(datasets[index_deep_dive_dataset].get('dataset').get('@id'))\n",
    "print(datasets[index_deep_dive_dataset].get('dataset').get('name'))\n",
    "data_df_deep_dive_name = datasets[index_deep_dive_dataset].get('dataset').get('name')\n",
    "print(datasets[index_deep_dive_dataset].get('viewUrl'))\n",
    "\n",
    "# display the pandas dataframe containing the datafile\n",
    "display(data_df_deep_dive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818801c-d836-4bc8-8543-a7f4c21e33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise: can load any data that you downloaded previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c23323-6c77-47cc-9b2a-d77c8941d8e2",
   "metadata": {},
   "source": [
    "### 2. Look at basic statistics and data coverage\n",
    "\n",
    "Print out the basic statistics of the variables, as well as the date range for both dataset files. <br>\n",
    "By gleaning more information - we can begin to determine which dataset may be useful for our science question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949a3ff-1926-43e5-87a6-ddfb63788c8b",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ce41a-6a74-4f90-90e8-41b053f4f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_df in ['data_df_datasetapi','data_df_deep_dive']:\n",
    "    print(vars()[str(data_df)+'_name'])\n",
    "    date_range = (vars()[data_df]['DateTime'].min(), vars()[data_df]['DateTime'].max())\n",
    "    print(f\"Date range: {date_range[0]} to {date_range[1]}\")\n",
    "    display(vars()[data_df].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ffd50",
   "metadata": {},
   "source": [
    "#### They both look interesting. Let's plot both!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8823371-f416-4fdc-a44c-a6fbb3dc2e25",
   "metadata": {},
   "source": [
    "### 3. Plot the data to visualize basic patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ab98b-0d28-4d53-9c11-10cda219f546",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17035d-6117-4c90-84e7-6171ac4e4a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATASET API RESULT\n",
    "\n",
    "# Select the dataset you want to plot\n",
    "dataframe = data_df_datasetapi\n",
    "\n",
    "# Select the variables that you are interest in plotting\n",
    "variables_of_interest = ['Temperature','Dissolved_Oxygen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f054ba-1469-4f16-8458-5e4f7eaf799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DEEP DIVE API RESULT\n",
    "\n",
    "# # Select the dataset you want to plot\n",
    "# dataframe = data_df_deep_dive\n",
    "\n",
    "# # Select the variables that you are interest in plotting\n",
    "# variables_of_interest = ['Temperature','Specific_Conductance']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31fc798-c8c8-4559-87c2-42ae306afea0",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749f66d-5174-4581-bd99-173c60447443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "# ===================================\n",
    "# Convert 'DateTime'to datetime using:\n",
    "dataframe['DateTime'] = pd.to_datetime(dataframe['DateTime'])\n",
    "\n",
    "num_plots = len(variables_of_interest)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(num_plots, 1, figsize=(10, 8))\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    # Plot VARIABLE over time\n",
    "    ax.plot(dataframe['DateTime'], dataframe[variables_of_interest[i]], label=variables_of_interest[i])\n",
    "    ax.set_title(variables_of_interest[i] + ' over Time')\n",
    "    ax.set_xlabel('DateTime')\n",
    "    ax.set_ylabel(variables_of_interest[i])\n",
    "    ax.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc3f34",
   "metadata": {},
   "source": [
    "### Using this plot, we can visually see the data coverage, and start to think about patterns in the data.\n",
    "### Visualizing the data can help you determine if this data file may work for your science question. You can keep going with analysis by inserting your custom analysis code here! Or, you can move on to the next section and download the data for future use.\n",
    "RESOURCE: [Python pandas user guide](https://pandas.pydata.org/docs/user_guide/index.html#user-guide) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15602598",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f5fab",
   "metadata": {},
   "source": [
    "# Part 4: Download Files and Save the Download Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e8c2b",
   "metadata": {},
   "source": [
    "## A. Download file(s) to local directory\n",
    "If desired, change save location and file location.\n",
    "Otherwise the path configured at the begining of the notebook will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71224e3",
   "metadata": {},
   "source": [
    "### 1. Ensure you have the right file to download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc270dc6-d076-41d9-b63d-be385bfd914d",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed06c73-50bc-41df-9e9e-902f9c5a75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to view dataset to check if this is the one you want to download\n",
    "data_df_datasetapi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63aa778-9a99-4fcc-91b5-b13f5170bd2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run cell to view dataset to check if this is the one you want to download\n",
    "data_df_deep_dive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c0ec7-4ad5-4aab-86f2-b8dd4eb4d072",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de46081-b4f5-4b36-8626-68f8fd722509",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_to_download = \"data_df_datasetapi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f29daf",
   "metadata": {},
   "source": [
    "### 2. Download the file and update the file download log.\n",
    "#### This example will download the whole zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc0884c-8236-43f3-83c6-919cb23923eb",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b684c-5390-4727-bddf-fb0a9b9e62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zip file to the chosen directory\n",
    "if datafile_to_download == \"data_df_datasetapi\":\n",
    "    file_indices = [zip_file_index]\n",
    "    dataset_details_chosen =  no_flmd_datasets[ds_idx_no_flmd]\n",
    "    dataset_citation = citations_list.get(dataset_details_chosen.get('@id'))\n",
    "    \n",
    "    ds_doi = download_selected_files(dataset_details_chosen, file_indices, download_dir_path,citation=dataset_citation) \n",
    "\n",
    "if datafile_to_download == \"data_df_deep_dive\":\n",
    "    dataset_details_chosen =  dataset_details[total_doi_array.index(current_response_json.get('doi'))]\n",
    "\n",
    "    files_deep_dive = dataset_details[total_doi_array.index(current_response_json.get('doi'))].get('distribution')\n",
    "    zipfile_to_download = current_response_json.get('data_file').split('/', 1)[0]\n",
    "    index = next((i for i, item in enumerate(files_deep_dive) if item['name'] == zipfile_to_download), None)\n",
    "    file_indices = [index]\n",
    "\n",
    "    file_to_download = current_response_json.get('data_file').rsplit('/', 1)[-1]\n",
    "\n",
    "    dataset_citation = citations_list.get(current_response_json.get('doi'))\n",
    "\n",
    "    ds_doi = download_selected_files(dataset_details_chosen, file_indices, download_dir_path, citation=dataset_citation) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1523962",
   "metadata": {},
   "source": [
    "#### You can view the Download log file to see a list of the files that we downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172064b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional: display the whole download file log\n",
    "# ===================================\n",
    "display(download_file_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f69270",
   "metadata": {},
   "source": [
    "### 3. Download the Download File Log to get a list of citations of data that we downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1268729-8615-4934-af50-06093c7c1ace",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = 'essdive_downloaded_files_log.csv'\n",
    "log_fn_path = download_dir_path / log_filename\n",
    "\n",
    "with open(log_fn_path, mode='w') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(['dataset_id', 'file_name', 'access_datetime', 'access_url', 'dataset_name', 'citation'])\n",
    "    \n",
    "    for ds_id, log_info in download_file_log.items():\n",
    "        ds_name = log_info.get('name')\n",
    "        ds_citation = log_info.get('citation')\n",
    "        \n",
    "        accessed_file_list = log_info.get('downloaded_files')\n",
    "        for accessed_file in accessed_file_list:\n",
    "            fn, fn_url, access_ts = accessed_file\n",
    "            \n",
    "            csv_writer.writerow([ds_id, fn, access_ts, fn_url, ds_name, ds_citation])\n",
    "            \n",
    "print(f'Check {str(download_dir_path)} for the log file: {log_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e9b0a",
   "metadata": {},
   "source": [
    "# That's a wrap!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca803ce3",
   "metadata": {},
   "source": [
    "----- \n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dcecbd-2ce5-4898-9996-f7a781d555fa",
   "metadata": {},
   "source": [
    "# Part 5. Extra Resources and Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96135178-234b-4b3d-ae77-b01cf855b9d3",
   "metadata": {},
   "source": [
    "## A. Manually inspecting the FLMD and Data Dictionary (DD)\n",
    "### (Part 2. Exploring inside datasets)\n",
    "This section manually examines structured data (FLMD) through FLMD and DD, which may be useful for a variety of purposes. This may be an alternative to the Deep Dive, in case there are files that are not in the Fusion database.  <br>\n",
    "**_Note_**: This section is an alternative to **[Part 2: Exploring, Section B: Advanced Search](##B-Advanced-Search-Look-at-the-datasets-API-Tools)** will be skipped in the 2024 Workshop Tutorial, but is available here for you to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5252708a",
   "metadata": {},
   "source": [
    "### 1. View datasets with accessible FLMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae873c73-e801-4c91-9d38-9e0e1f95d4b4",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff2bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, fd in enumerate(flmd_datasets):\n",
    "    print(f\"--- Index {idx}: {fd.get('name')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825e83b",
   "metadata": {},
   "source": [
    "### 2. Choose dataset to inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec267c4-c56e-4be0-840a-17fea8dd5b7f",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bce5a-c00a-48da-91cb-736b74445388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write in the index of the FLMD dataset you want to investigate\n",
    "ds_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47009de-68a2-4379-ac70-31a77469ce92",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = flmd_datasets[ds_idx]\n",
    "\n",
    "# helper function to print the dataset information\n",
    "def print_dataset_info(d, info_fields=['@id', 'name', 'description', 'citation'], line_space=False):\n",
    "    \"\"\" \n",
    "    Display basic dataset info for evaluation \n",
    "    \"\"\"\n",
    "    for f in info_fields:\n",
    "        value = d.get(f)\n",
    "        if value is None:\n",
    "            dataset_value = d.get('dataset')\n",
    "            if dataset_value:\n",
    "                value = dataset_value.get(f)  \n",
    "        if value:\n",
    "            if f in ['flmd_url', 'csv_files']:\n",
    "                print(f\"--- {f}:\")\n",
    "                for filename, url in value.items():\n",
    "                    print(f\"    - {filename}\")\n",
    "                continue      \n",
    "            print(f\"--- {f}: {value}\")\n",
    "            if line_space:\n",
    "                print(\" \")\n",
    "\n",
    "print_dataset_info(dataset, info_fields=['@id', 'name', 'flmd_url'], line_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aea0c9",
   "metadata": {},
   "source": [
    "### 3. Select and read flmd\n",
    "\n",
    "_If multiple flmd files exist in the dataset, run the cell below as many times as needed changing the index._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f74af9-61f0-4be7-a0bd-2d061b863c5a",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d941e-f69e-4f19-ab9c-99b2eeb2aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select index of the FLMD you want to use\n",
    "flmd_file_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ef402-c79a-44a6-8363-9aadd1c2a446",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e546ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the flmd\n",
    "flmd_name, flmd_url = list(dataset.get('flmd_url').items())[flmd_file_idx]\n",
    "print(f\"{flmd_name}: {flmd_url}\")\n",
    "print('-------------------------')\n",
    "\n",
    "flmd_response = get_request(flmd_name, flmd_url)\n",
    "\n",
    "flmd_headers, flmd_store = make_store(flmd_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa50ff",
   "metadata": {},
   "source": [
    "### 4. View dataset files listed in flmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d4d65-7c56-4f4c-886e-5f476a0b567f",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e3d01-7154-4c7c-9fa3-3178853362ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter flmd fields to view (File name automatically included):\n",
    "flmd_header_indices = [1, -2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238a81e-0029-4d0c-b11f-cbc3a15233b4",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1b21d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print dataset files in flmd\n",
    "for idx, flmd_info in flmd_store.items():\n",
    "    print(f\"{idx}: {flmd_info.get(flmd_headers[0])}\")\n",
    "    for flmd_idx in flmd_header_indices:\n",
    "        print(f\"-- {flmd_headers[flmd_idx]}: {flmd_info.get(flmd_headers[flmd_idx])}\")\n",
    "    print(f\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d45fa",
   "metadata": {},
   "source": [
    "### 5. Inspect dataset file contents using Data Dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00808449-401b-4ada-a064-688dadd8fba6",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f590572-cb16-4fa9-9d4b-7f9a7e43c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter data file index\n",
    "data_file_index = 7\n",
    "\n",
    "# Enter Data Dictionary file index\n",
    "dd_file_index = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2522a55-4b2c-4046-be2a-e22121dd8452",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8539c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the DD\n",
    "dd_file_name = flmd_store[f\"Index {dd_file_index}\"].get('File_Name')\n",
    "data_file_name = flmd_store[f\"Index {data_file_index}\"].get('File_Name')\n",
    "print(f'Data File: {data_file_name}\\n'\n",
    "      f'Data Dictionary File: {dd_file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57998e82-68d9-4740-ad71-30142c4c5c9c",
   "metadata": {},
   "source": [
    "### 6. Check if the DD is zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c989fb-204e-4f48-a80c-f7e5ca781f78",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc68277-cf57-4335-b4a9-6548ccc4ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which files you want to print out that are included in the dataset\n",
    "file_type = 'all'  # 'all' or 'csv' or 'pdf' or 'zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718f362-0b10-4567-94f3-5f85234105a2",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd823107-4b42-40b5-9dab-9d39e1e7f8e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function that lists the files included\n",
    "inspect_dataset_distribution(dataset, file_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae892df-4871-4286-b733-09bc6803d2ea",
   "metadata": {},
   "source": [
    "### 7A) IF DD in zip: search in zip for DD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7920b-0414-4aa8-9250-527baef59668",
   "metadata": {},
   "source": [
    "#### 1. Show zip contents to select DD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825971f2-81f9-4eb1-a6f3-e220f32a029e",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ddbe0-69ac-486e-b07e-5dc0d43d6bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file from file distribution - choose the zip where you think the DD may be\n",
    "zip_file_idx = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75275a5-39cf-47bd-b226-3eff95d5a655",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef887f-b749-4cf5-9078-ca0e18375d90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function that prints zipped file content\n",
    "fn, zip_download = inspect_zip_file_contents(dataset, zip_file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0de4c2-e3c8-4de6-96f0-90c28d771379",
   "metadata": {},
   "source": [
    "#### 2. Display DD within zip file to inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe6165-070f-4b12-bb5b-1cb4e1821b1c",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:blue\">Enter INPUT</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121e5d6-a88f-4e77-8927-8678c2b20e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if csv file is zipped up\n",
    "dd_csv = 0\n",
    "\n",
    "# If needed adjust the number of rows to skip.\n",
    "header_rows = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0aee3-5537-467c-a206-66d233632e57",
   "metadata": {},
   "source": [
    "<strong><span style=\"color:green\">Run Cell</span></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7e785-30db-4239-9ae1-b331039a065d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_file_name = zip_download.namelist()[dd_csv]\n",
    "print(f'Attempting to read: {csv_file_name} from zip file {fn}')\n",
    "\n",
    "metadata_df = read_zipped_csv(zip_download, csv_file_name, header_rows)\n",
    "zip_download_dd = zip_download\n",
    "fn_dd = fn\n",
    "\n",
    "if metadata_df is not None:\n",
    "    is_csv_zipped = True\n",
    "    headers = list(metadata_df.columns)\n",
    "    display(metadata_df)\n",
    "else:\n",
    "    print('ERROR: Sample metadata file was not successfully loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c5bfe",
   "metadata": {},
   "source": [
    "### 7B) If DD not in zip: Inspect data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "data_files = dataset.get('csv_files')\n",
    "\n",
    "if dd_file_name not in data_files.keys():\n",
    "    print(f\"Cannot find {dd_file_name} in dataset distribution.\")\n",
    "else:\n",
    "    dd_url = data_files[dd_file_name]\n",
    "    print(f\"{dd_file_name}\")\n",
    "    print(f\"{dd_url}\")\n",
    "    print('-------------------------')\n",
    "\n",
    "    dd_request = get_request(dd_file_name, dd_url)\n",
    "    dd_headers, dd_store = make_store(dd_request)\n",
    "    print('-------------------------')\n",
    "\n",
    "    for idx, dd_info in dd_store.items():\n",
    "        print(f\"{dd_info.get(dd_headers[0])} -- Units: {dd_info.get(dd_headers[1])} -- Desc: {dd_info.get(dd_headers[2])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f796a-1532-43a6-baf8-02f3571df333",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1ac57-4fed-4c71-8ca2-06ae7b754af2",
   "metadata": {},
   "source": [
    "## B. Finding data using Sample ID and Metadata Reporting Formats - workflow\n",
    "\n",
    "### Tutorial_FindingAccessData.ipynb - 2023 ESS-DIVE Community Workshop\n",
    "This notebook [Tutorial_FindingAccessData.ipynb](https://github.com/ess-dive/essdive-tutorials/blob/main/search_data/Tutorial_FindingAccessingData.ipynb) is from the Finding and Accessing Data Tutorial 2023. It contains a similar workflow to this notebook (albeit without the Deep Dive API), but also additional information and code including:\n",
    "\n",
    "1. (Step 6 of DSC's notebook) Using Sample ID and Metadata Reporting Formats\n",
    "   - The example utilizes data that contain the Sample ID reporting formats.\n",
    "   - It utilizes the same basic tools: Dataset API, inspecting reporting format files, etc to provide another way to utilize ESS-DIVE data\n",
    "   - You will want to run Steps 1: Set Up before running Step 6: Sample ID and Metadata Reporting Formats. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stml",
   "language": "python",
   "name": "stml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
