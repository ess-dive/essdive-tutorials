{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ffe901",
   "metadata": {},
   "source": [
    "# Welcome to ESS-DIVE's Finding & Accessing Data Jupyter Notebook\n",
    "\n",
    "This Jupyter Notebook will help data users find and access ESS-DIVE datasets that employ file-level metadata and csv reporting formats, including:\n",
    "\n",
    "    Use the ESS-DIVE Dataset API to access dataset files\n",
    "    Use the xml file to explore / access a dataset\n",
    "    Use the File-level Metadata (flmd) to explore the dataset\n",
    "    Use Data Dictionaries to understand data content\n",
    "    Explore Sample Metadata to explore datasets with sample-based data\n",
    "    Import data from csv files into python pandas dataframes\n",
    "    Download files to local storage and log access details\n",
    "\n",
    "This was created as a resource to the ESS-DIVE 2023 Open Data Workshop.\n",
    "\n",
    "Written By: Danielle S Christianson (she/her, LBNL)\n",
    "\n",
    "Acknowledgements: This notebook builds from Madison Burrus and Valerie Hendrix's Search & Download notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc060b07",
   "metadata": {},
   "source": [
    "## README: How to use this notebook\n",
    "\n",
    "Enter input and/or read information above the \"# ===============================\" in python cells. \n",
    "Otherwise, just run the cell.\n",
    "\n",
    "Optional view cells are marked with \"Optional\" in the first line. These do not need to be run.\n",
    "\n",
    "Any downloaded files are logged with the date/time of access. See Section 7 to save the log.\n",
    "\n",
    "Workflows:\n",
    "* Cells in Section 1-5 are sequential and depend on variables entered in prior cells. \n",
    "* To use Section 6: Sample ID and Metadata, run Section 1: Setup, then proceed to Section 6.\n",
    "* Section 7: Saving the download log file can be run after Section 5 or after Section 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1686044",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7213b9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This notebook requires Python 3.\n",
    "# ===================================\n",
    "\n",
    "import csv\n",
    "import datetime as dt\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from ipywidgets import widgets, interact\n",
    "from IPython.display import display, display_html\n",
    "from pathlib import Path\n",
    "from urllib.request import Request, urlopen, urlretrieve\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cc627",
   "metadata": {},
   "source": [
    "### Configure authentification\n",
    "\n",
    "1. Go to ESS-DIVE (https://data.ess-dive.lbl.gov/data), login with your ORCID, and copy your authentication token from your account settings page.\n",
    "2. Enter your authentication token into the widget above\n",
    "3. Run the following code cell\n",
    "\n",
    "   _Always re-run this code cell when you update your token. Tokens expire every 24 hours._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_token = \"<put_your_token_here>\"\n",
    "\n",
    "# ===================================\n",
    "token_text = widgets.Text(my_token, description=\"Token:\")\n",
    "display(token_text)\n",
    "\n",
    "token = token_text.value\n",
    "essdive_api_url = 'https://api.ess-dive.lbl.gov'\n",
    "\n",
    "essdive_direct_url = 'https://data.ess-dive.lbl.gov/catalog/d1/mn/v2/object/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9950916",
   "metadata": {},
   "source": [
    "### Configure local storage for downloads (if desired)\n",
    "\n",
    "Enter the local directory path in which you want to save downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72008418",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = Path('<put_local_directory_here>')\n",
    "\n",
    "# ===================================\n",
    "if local_dir.exists():\n",
    "    print(f'Success! Local directory {local_dir} configured for downloads')\n",
    "    print('===================================')\n",
    "    current_files = [x for x in os.listdir(local_dir) if x != '.DS_Store']\n",
    "    if current_files:\n",
    "        print(f'Local directory contains: {current_files}')\n",
    "    else:\n",
    "        print(f'Local directory is currently empty.')\n",
    "else:\n",
    "    print(f'Cannot find local directory {local_dir}. Please reenter valid directory path.')\n",
    "    \n",
    "download_file_log = {}\n",
    "print('===================================')\n",
    "print('Downloaded files will be logged in the dictionary object \"download_file_log\".\\n'\n",
    "      'You can save this dictionary as a file later in the notebook.\\n'\n",
    "      'The filename, file url, and datetime accessed are recorded as a tuple in the \"downloaded_files\" element.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33756161",
   "metadata": {},
   "source": [
    "### Load general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc486f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these general functions\n",
    "# ===================================\n",
    "\n",
    "def print_dataset_info(d, info_fields=['@id', 'name', 'description', 'citation'], line_space=False):\n",
    "    \"\"\" \n",
    "    Display basic dataset info for evaluation \n",
    "    \"\"\"\n",
    "    for f in info_fields:\n",
    "        value = d.get(f)\n",
    "        \n",
    "        if value is None:\n",
    "            dataset_value = d.get('dataset')\n",
    "            if dataset_value:\n",
    "                value = dataset_value.get(f)\n",
    "                    \n",
    "        if value:\n",
    "            if f in ['flmd_url', 'csv_files']:\n",
    "                print(f\"--- {f}:\")\n",
    "                for filename, url in value.items():\n",
    "                    print(f\"    - {filename}\")\n",
    "                continue\n",
    "                          \n",
    "            print(f\"--- {f}: {value}\")\n",
    "            if line_space:\n",
    "                print(\" \")\n",
    "\n",
    "\n",
    "def print_datasets_info(dataset_list, info_fields=['@id', 'name', 'description', 'citation'], line_space=False):\n",
    "    \"\"\" \n",
    "    Display basic dataset info for evaluation \n",
    "    \"\"\"\n",
    "    print(f'=========== Info for {len(dataset_list)} datasets ===========')\n",
    "    for a_dataset in dataset_list:\n",
    "        print_dataset_info(a_dataset, info_fields, line_space)\n",
    "                \n",
    "        print(\"----------------------------------------------------------\")\n",
    "        \n",
    "        \n",
    "        \n",
    "def assess_datasets_flmd_dd_csv_files(dataset_details_list):\n",
    "    \"\"\"\n",
    "    Find the datasets with flmd files\n",
    "    Sort the csv file contents into potential and data files; add to the dataset details dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    flmd_datasets_indices = set()\n",
    "    flmd_dataset_details = []\n",
    "    \n",
    "    for idx, dataset in enumerate(dataset_details_list):\n",
    "        file_list = dataset.get('distribution')\n",
    "    \n",
    "        flmd_url = {}\n",
    "        csv_files = {}\n",
    "        for f in file_list:\n",
    "            encoding_format = f.get('encodingFormat')\n",
    "            filename = f.get('name')\n",
    "            url = f.get('contentUrl')\n",
    "        \n",
    "            if 'csv' not in encoding_format or url is None:\n",
    "                continue\n",
    "        \n",
    "            if 'flmd' in filename:\n",
    "                flmd_datasets_indices.add(idx)\n",
    "                flmd_url.update({filename: url})\n",
    "        \n",
    "            else:\n",
    "                csv_files.update({filename: url})\n",
    "\n",
    "        dataset.update({\n",
    "            'flmd_url': flmd_url,\n",
    "            'csv_files': csv_files\n",
    "        })\n",
    "    \n",
    "        if not flmd_url:      \n",
    "            dataset_name = dataset.get('name')\n",
    "            print(f\"No flmd found for dataset: {dataset_name}\")\n",
    "        \n",
    "    print(\"=====================================\")\n",
    "    \n",
    "    if len(flmd_datasets_indices) > 0:\n",
    "        print(f'flmd found in {len(flmd_datasets_indices)} datasets')\n",
    "        flmd_dataset_details = [dataset_details_list[x] for x in flmd_datasets_indices]\n",
    "    else:\n",
    "        print(f'No datasets in the search results have flmds.')\n",
    "        \n",
    "    no_flmd_dataset_details = [dataset_detail for idx, dataset_detail in enumerate(dataset_details_list) if idx not in flmd_datasets_indices]\n",
    "    \n",
    "    return flmd_dataset_details, no_flmd_dataset_details\n",
    "\n",
    "\n",
    "def get_dataset_details(dataset_url):\n",
    "    \n",
    "    response_status = None\n",
    "    try:\n",
    "        dataset_response = requests.get(dataset_url, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "        response_status = dataset_response.status_code\n",
    "    except Exception as e:\n",
    "        print(f\"{dataset.get('dataset').get('name')} did not have a successful return: {e}\")\n",
    "        return None\n",
    "\n",
    "    # If successful response, add to dataset_store\n",
    "    if response_status == 200:\n",
    "            dataset_json = dataset_response.json()['dataset'] \n",
    "            print(f\"--- Acquired details for {dataset_json.get('name')}\")\n",
    "            return dataset_json\n",
    "    elif response_status:  \n",
    "        print(f\"Response status {response_status}: {dataset_response.text}\")\n",
    "    else:\n",
    "        print(f\"Response status unavailable. Response cannot be interpreted. Debug required.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_request(filename, f_url, stream=True):\n",
    "    \"\"\"\n",
    "    Get request for file, and stream the content back\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {'user_agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "               'content-type': 'application/json'}\n",
    "    try:\n",
    "        r = requests.get(f_url, headers=headers, verify=True, stream=stream)\n",
    "        status_code = r.status_code\n",
    "        if status_code == 200:\n",
    "            return r\n",
    "        else:\n",
    "            print(f\"{filename} request returned {status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"{filename} request unsuccessful: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def make_store(file_request, use_idx=True, print_headers=True):\n",
    "    \"\"\"\n",
    "    Read response and make store\n",
    "    \"\"\"\n",
    "    file_store = {}\n",
    "    csv_reader = csv.DictReader(file_request.iter_lines(decode_unicode=True))\n",
    "\n",
    "    for idx, row in enumerate(csv_reader):\n",
    "        if use_idx:\n",
    "            file_store.update({f'Index {idx}': row})\n",
    "            continue\n",
    "        fn = row.get('File_Name')\n",
    "        file_store.update({fn: row})\n",
    "    \n",
    "    headers = list(row.keys())\n",
    "    if print_headers:\n",
    "        print(f\"File headers: {headers}\")\n",
    "    return headers, file_store\n",
    "\n",
    "\n",
    "def make_pandas_df(file_url, header_rows=1, print_headers=True):\n",
    "    \"\"\"\n",
    "    Read response and make pandas pdf from online csv file\n",
    "    Designed for ESS-DIVE Sample ID and Metadata RF sample_metadata.csv files that have one header row.\n",
    "    \"\"\"\n",
    "    p_df = pd.read_csv(file_url, skiprows=header_rows)\n",
    "    \n",
    "    headers = list(p_df.columns)\n",
    "    if print_headers:\n",
    "        print(f\"File headers: {headers}\")\n",
    "    return headers, p_df\n",
    "\n",
    "\n",
    "def inspect_dataset_distribution(dataset_detail, file_type='all'):\n",
    "\n",
    "    print(dataset_detail.get('name'))\n",
    "    print('========================================')\n",
    "\n",
    "    count = 0\n",
    "    dist = dataset_detail.get('distribution')\n",
    "    \n",
    "    for idx, file_info in enumerate(dist):\n",
    "        fn = file_info.get('name')\n",
    "        fn_url = file_info.get('contentUrl')\n",
    "        f_encoding = file_info.get('encodingFormat')\n",
    "        if file_type != 'all' and file_type not in f_encoding:\n",
    "            continue\n",
    "        print(f'Index {idx}: {fn}\\n  encoding: {f_encoding}\\n  url: {fn_url}')\n",
    "        count += 1\n",
    "        \n",
    "    if count == 0:\n",
    "        print(f'No files found that match the file_type: \"{file_type}\" criteria.')\n",
    "            \n",
    "            \n",
    "def retrieve_file_from_essdive(file_url, file_path):\n",
    "    \"\"\" Retrieve the data file \n",
    "        file_path includes file name.\n",
    "    \"\"\"     \n",
    "    try:\n",
    "        urlretrieve(file_url, file_path)\n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        return False, f'File at url: {file_url} was not saved: {e}'\n",
    "    \n",
    "\n",
    "def download_selected_files(dataset_detail, file_indices, file_dir=local_dir, log_store=download_file_log, \n",
    "                            is_csv_zipped=False, zip_download=None, zip_member_fn=None):\n",
    "    dist = dataset_detail.get('distribution')\n",
    "    ds_id = dataset_detail.get('@id')\n",
    "    citation = dataset_detail.get('citation')\n",
    "    ds_name = dataset_detail.get('name')\n",
    "    \n",
    "    if log_store is None:\n",
    "        log_store = {}\n",
    "    \n",
    "    log_store.setdefault(ds_id, {'@id': ds_id, 'name': ds_name, 'citation': citation, 'downloaded_files': []})\n",
    "    ds_file_log = log_store.get(ds_id).get('downloaded_files')\n",
    "    \n",
    "    print(f'Saving files in {local_dir}')\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "    for idx, file_info in enumerate(dist):\n",
    "        msg = None\n",
    "        is_downloaded = None\n",
    "        \n",
    "        if idx not in file_indices:\n",
    "            continue\n",
    "            \n",
    "        fn = file_info.get('name')\n",
    "        file_path = local_dir / fn\n",
    "        fn_url = file_info.get('contentUrl')\n",
    "        \n",
    "        if not is_csv_zipped:\n",
    "    \n",
    "            download_ts = dt.datetime.now().isoformat()\n",
    "            is_downloaded, msg = retrieve_file_from_essdive(fn_url, file_path)\n",
    "    \n",
    "        else:\n",
    "            if not zip_download or not zip_member_fn:\n",
    "                print('ZipFile object and zipped member file name are required. Try again.')\n",
    "                return None\n",
    "            try:\n",
    "                zip_download.extract(zip_member_fn, path=file_path)\n",
    "                if Path.exists(file_path / zip_member_fn):\n",
    "                    is_downloaded = True\n",
    "                    download_ts = dt.datetime.now().isoformat()\n",
    "                else:\n",
    "                    msg = f'Extraction of {zip_member_fn} from {fn} was not successful.'\n",
    "            except Exception as e:\n",
    "                msg = f'ERROR attempting to extract {zip_member_fn} from {fn}: {e}'\n",
    "        \n",
    "        if is_downloaded:\n",
    "            print(f'--- {fn} downloaded')\n",
    "            ds_file_log.append((fn, fn_url, download_ts))\n",
    "        else:\n",
    "            print(msg)\n",
    "            \n",
    "    print(\"-------------------------------------\")\n",
    "    print(f'Remember to cite these files! Dataset DOI {ds_id}')\n",
    "    return ds_id    \n",
    "\n",
    "\n",
    "def inspect_zip_file_contents(dataset_detail, file_idx):\n",
    "    dist = dataset_detail.get('distribution')\n",
    "    file_info = dist[7]\n",
    "    \n",
    "    if not file_info:\n",
    "        print('File index not found. Please try again.')\n",
    "        return\n",
    "    \n",
    "    fn = file_info.get('name')\n",
    "    if 'zip' not in file_info.get('encodingFormat'):\n",
    "        print(f'{fn} is not encoded as a zip file. Please select a different file.')\n",
    "    \n",
    "    fn_url = file_info.get('contentUrl')\n",
    "    resp = urlopen(fn_url)\n",
    "    \n",
    "    zip_download = ZipFile(io.BytesIO(resp.read()))\n",
    "    \n",
    "    print(f'{fn} contents:')\n",
    "    print('=================================')\n",
    "    for idx, file_member in enumerate(zip_download.namelist()):\n",
    "        print(f'Index {idx}: {file_member}')\n",
    "        \n",
    "    return fn, zip_download\n",
    "\n",
    "\n",
    "def read_zipped_csv(zip_file_obj, csv_file_name, header_rows=1):\n",
    "    # with open(zip_file_obj, mode='r') as z:\n",
    "    #     csv_df = pd.read_csv(io.BytesIO(z.read(csv_file_name)))\n",
    "    csv_df = pd.read_csv(zip_download.open(csv_file_name), skiprows=header_rows)\n",
    "    return csv_df\n",
    "    \n",
    "    \n",
    "print('Functions loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff350720",
   "metadata": {},
   "source": [
    "# 2. Search ESS-DIVE using Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc721ea",
   "metadata": {},
   "source": [
    "Use the ESS-DIVE Dataset API to search for datasets of interest.\n",
    "\n",
    "You can search for datasets using any of the following parameters:\n",
    "- Dataset Creator (creator)\n",
    "- Date Published (datePublished)\n",
    "- Project Name (providerName)\n",
    "- Any text (text)\n",
    "- Keywords (keywords)\n",
    "- Public datasets only (isPublic)\n",
    "\n",
    "**See additional details for dataset search in the ESS-DIVE package API techincal documentation:**** https://api.ess-dive.lbl.gov/#/Data%20Package/listPackages.\n",
    "\n",
    "Use the [ESS-DIVE's project list](https://docs.google.com/spreadsheets/d/179SOyv42wXbP4owWZtUg3RqhW9dPOyENYcVYuUCcqwg/edit?usp=sharing) to find the options for project names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter search terms\n",
    "# For an exact match, put the string in quotes, e.g. \"\\\"Leaf\"\\\" is an exact match, \"Leaf\" is any match\n",
    "\n",
    "providerName=\"\\\"Next-Generation Ecosystem Experiments (NGEE) Arctic\\\"\"  # \"\\\"<project name>\\\"\"\n",
    "creator=\"Ely\"\n",
    "text= \"\\\"Leaf\\\"\"\n",
    "datePublished = \"[2016 TO 2023]\"  # \"<[YYYY TO YYYY-MM-DD]>\" # Not the same as data coverage\n",
    "\n",
    "# ===================================\n",
    "# ToDo: make construction of URL a function to handle empty search criteria\n",
    "\n",
    "# Contruct URL query to send to the ESS-DIVE packages API\n",
    "get_packages_response = f\"{essdive_api_url}/packages?providerName={providerName}&creator={creator}&text={text}&datePublished={datePublished}&isPublic=true\"\n",
    "\n",
    "# Send request to API\n",
    "response = requests.get(get_packages_response, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "\n",
    "# Review the response and debug if needed\n",
    "if response.status_code == 200:\n",
    "    # Success\n",
    "    response_json = response.json()\n",
    "    print(\"Success! Continue to look at the search results\")  \n",
    "else:\n",
    "    # There was an error\n",
    "    print(\"There was an error. Stop here and debug the issue. Email ess-dive-support@lbl.gov if you need assistance. \\n\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b678f5",
   "metadata": {},
   "source": [
    "### Inspect the search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "search_record_total = response_json['total']\n",
    "print(f\"Datasets found: {search_record_total}\")\n",
    "\n",
    "if search_record_total > 100:\n",
    "    print(\"The search API cannot return more than 100 results at a time. See documentation for how to paginate.\")\n",
    "\n",
    "candidate_datasets = response_json['result']\n",
    "\n",
    "for idx, dataset in enumerate(candidate_datasets):\n",
    "    print('-------------------')\n",
    "    print(f'Index: {idx}')\n",
    "    print(dataset.get('dataset').get('name'))\n",
    "    print(dataset.get('url'))\n",
    "    print(dataset.get('viewUrl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: display entire response\n",
    "# ===================================\n",
    "display(response_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb3ec5",
   "metadata": {},
   "source": [
    "## Subset search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_indices = [6, 4, 5, 7]\n",
    "\n",
    "# ===================================\n",
    "datasets = [candidate_datasets[x] for x in record_indices]\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    print(f\"{idx}: {dataset.get('dataset').get('name')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c59c8",
   "metadata": {},
   "source": [
    "### Get dataset details using ESS-DIVE Dataset API\n",
    "\n",
    "Use the ESS-DIVE individual dataset search to get details of the datasets, including its list of files.\n",
    "\n",
    "The results of the above search contain the URLs to retrieve the dataset details in the field: `url`.\n",
    "\n",
    "**See more details for the individual dataset search in the ESS-DIVE package API techincal documentation:** https://api.ess-dive.lbl.gov/#/Dataset/getDataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Store the dataset details in a list\n",
    "dataset_details = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_url = dataset.get('url')\n",
    "    dataset_detail_json = get_dataset_details(dataset_url)\n",
    "    if dataset_detail_json:\n",
    "        dataset_details.append(dataset_detail_json) \n",
    "\n",
    "print(\"=====================================\")\n",
    "print(f\"Details acquired for {len(dataset_details)} datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: display dataset information\n",
    "# (Un)comment options below\n",
    "\n",
    "# print_datasets_info(dataset_details)\n",
    "display(dataset_details[2])\n",
    "\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739c67e",
   "metadata": {},
   "source": [
    "### Determine which datasets have flmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27036b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "flmd_datasets, no_flmd_datasets = assess_datasets_flmd_dd_csv_files(dataset_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28674de2",
   "metadata": {},
   "source": [
    "# 3. Inspect dataset using Dataset Details Distribution (without flmd)\n",
    "\n",
    "List the datasets that do not have flmd files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "for idx, fd in enumerate(no_flmd_datasets):\n",
    "    print(f\"--- Index {idx}: {fd.get('name')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a3e547",
   "metadata": {},
   "source": [
    "### Choose dataset to inspect using index above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9872076",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_id = 0\n",
    "file_type = 'all'  # 'all' or 'csv' or 'pdf' or 'zip'\n",
    "\n",
    "# ===================================\n",
    "inspect_dataset_distribution(no_flmd_datasets[ds_id], file_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ebc37",
   "metadata": {},
   "source": [
    "### Inspect the contents of a zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coming soon!! See Section 6 for example code and functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e8c2b",
   "metadata": {},
   "source": [
    "### Download file(s) to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_indices = [0, 2]\n",
    "\n",
    "# ===================================\n",
    "ds_doi = download_selected_files(no_flmd_datasets[ds_id], file_indices, local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ed053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: display the download file log for this DOI\n",
    "# ===================================\n",
    "print(f'Downloaded file information for {ds_doi}:')\n",
    "display(download_file_log[ds_doi])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa4b585",
   "metadata": {},
   "source": [
    "# 4. Inspect dataset contents using File-level Metadata (flmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5252708a",
   "metadata": {},
   "source": [
    "### View flmd datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff2bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "for idx, fd in enumerate(flmd_datasets):\n",
    "    print(f\"--- Index {idx}: {fd.get('name')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825e83b",
   "metadata": {},
   "source": [
    "### Choose dataset to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_id = 2\n",
    "\n",
    "# ===================================\n",
    "dataset = flmd_datasets[ds_id]\n",
    "print_dataset_info(dataset, info_fields=['@id', 'name', 'flmd_url'], line_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aea0c9",
   "metadata": {},
   "source": [
    "### Select and read flmd\n",
    "\n",
    "_If multiple flmd files exist in the dataset, run the cell below as many times as needed changing the index._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e546ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flmd_file_idx = 0\n",
    "\n",
    "# ===================================\n",
    "flmd_name, flmd_url = list(dataset.get('flmd_url').items())[flmd_file_idx]\n",
    "print(f\"{flmd_name}: {flmd_url}\")\n",
    "print('-------------------------')\n",
    "\n",
    "flmd_response = get_request(flmd_name, flmd_url)\n",
    "\n",
    "flmd_headers, flmd_store = make_store(flmd_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa50ff",
   "metadata": {},
   "source": [
    "### View dataset files listed in flmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flmd_header_indices = [1, -2]\n",
    "\n",
    "# ===================================\n",
    "for idx, flmd_info in flmd_store.items():\n",
    "    print(f\"{idx}: {flmd_info.get(flmd_headers[0])}\")\n",
    "    for flmd_idx in flmd_header_indices:\n",
    "        print(f\"-- {flmd_headers[flmd_idx]}: {flmd_info.get(flmd_headers[flmd_idx])}\")\n",
    "    print(f\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d45fa",
   "metadata": {},
   "source": [
    "# 5. Inspect dataset file contents using Data Dictionary\n",
    "\n",
    "### Choose indices of file of interest and its corresponding Data Dictionary file to inspect below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8539c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter data file index\n",
    "data_file_index = 2\n",
    "\n",
    "# Enter Data Dictionary file index\n",
    "dd_file_index = 3\n",
    "\n",
    "# ===================================\n",
    "dd_file_name = flmd_store[f\"Index {dd_file_index}\"].get('File_Name')\n",
    "data_file_name = flmd_store[f\"Index {data_file_index}\"].get('File_Name')\n",
    "print(f'Data File: {data_file_name}\\n'\n",
    "      f'Data Dictionary File: {dd_file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c5bfe",
   "metadata": {},
   "source": [
    "### Inspect data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "data_files = dataset.get('csv_files')\n",
    "\n",
    "if dd_file_name not in data_files.keys():\n",
    "    print(f\"Cannot find {dd_file_name} in dataset distribution.\")\n",
    "else:\n",
    "    dd_url = data_files[dd_file_name]\n",
    "    print(f\"{dd_file_name}\")\n",
    "    print(f\"{dd_url}\")\n",
    "    print('-------------------------')\n",
    "\n",
    "    dd_request = get_request(dd_file_name, dd_url)\n",
    "    dd_headers, dd_store = make_store(dd_request)\n",
    "    print('-------------------------')\n",
    "\n",
    "    for idx, dd_info in dd_store.items():\n",
    "        print(f\"{dd_info.get(dd_headers[0])} -- Units: {dd_info.get(dd_headers[1])} -- Desc: {dd_info.get(dd_headers[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0cc11",
   "metadata": {},
   "source": [
    "### Load selected csv data file into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "if data_file_name not in data_files.keys():\n",
    "    print(f\"Cannot find {data_file_name} in dataset distribution.\")\n",
    "else:\n",
    "    data_url = data_files[data_file_name]\n",
    "    print(f\"{data_file_name}\")\n",
    "    print(f\"{data_url}\")\n",
    "    print('-------------------------')\n",
    "\n",
    "    data_request = get_request(data_file_name, data_url, stream=False)\n",
    "    \n",
    "    data_df = pd.read_csv(io.StringIO(data_request.text))\n",
    "    \n",
    "    display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT your custom analysis code here\n",
    "# Python pandas user guide: https://pandas.pydata.org/docs/user_guide/index.html#user-guide "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215156b",
   "metadata": {},
   "source": [
    "### Downloal selected csv data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67023f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "ds_doi = download_selected_files(dataset, [data_file_index], local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273cd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: display the download file log for this DOI\n",
    "# ===================================\n",
    "print(f'Downloaded file information for {ds_doi}:')\n",
    "display(download_file_log[ds_doi])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5872c43",
   "metadata": {},
   "source": [
    "# 6. Use Sample ID and Metadata Reporting Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b874b",
   "metadata": {},
   "source": [
    "The example below starts with a search on the ESS-DIVE main search webpage: https://data.ess-dive.lbl.gov/\n",
    "\n",
    "The dataset version/identifier of the desired dataset is entered below as the dataset_id.\n",
    "* Find the dataset version in the upper left corner of the dataset's webpage next to the DOI.\n",
    "* Or find the dataset identifier as the first field in the General metadata section (below the dataset files).\n",
    "\n",
    "The search feature of the Dataset API illustrated in Section 2 above can also be used to find a dataset_id of interest. The dataset_id is the last part of the API URL shown in the results.\n",
    "\n",
    "Example:\n",
    "For the dataset detail url: https://api.ess-dive.lbl.gov/packages/ess-dive-f0861161a6bd3bf-20231109T125444193, the dataset_id is ess-dive-f0861161a6bd3bf-20231109T125444193.\n",
    "\n",
    "## README\n",
    "\n",
    "The code below performs **minimal** Sample ID and Metadata Reporting Format validation. Not all features may work if files do not adhere to the Reporting Format.\n",
    "\n",
    "*Note: We leave the sample_metadata.csv column names unvalidated to increase the ability of inspecting the files.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706e686",
   "metadata": {},
   "source": [
    "### Enter the dataset ID of interest\n",
    "\n",
    "Example datasets:\n",
    "* CSV files at top-level: ess-dive-2569191b32b447d-20230809T173212651\n",
    "* Zipped files at top-level: ess-dive-120a44f1c8a626c-20230914T183544541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'ess-dive-120a44f1c8a626c-20230914T183544541'\n",
    "\n",
    "# ===================================\n",
    "# Find dataset identifier from search above or via Search Webpage\n",
    "dataset_details_url = f'https://api.ess-dive.lbl.gov/packages/{dataset_id}'\n",
    "\n",
    "dataset_detail = get_dataset_details(dataset_details_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45909a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the dataset for fmld\n",
    "# ===================================\n",
    "flmd_datasets, no_flmd_datasets = assess_datasets_flmd_dd_csv_files([dataset_detail])\n",
    "\n",
    "# Additional setup\n",
    "# Set the default assumptions\n",
    "is_csv_zipped = False\n",
    "metadata_df = None\n",
    "igsn_col_idx = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7b68b",
   "metadata": {},
   "source": [
    "### View the dataset csv files\n",
    "\n",
    "Look for the sample metadata file.\n",
    "It is a csv file that should have \"sample_metadata\" in the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a926584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===================================\n",
    "csv_files = dataset_detail.get('csv_files')\n",
    "\n",
    "if not csv_files:\n",
    "    print('No csv files. Try Zip File Option below.')\n",
    "\n",
    "csv_index = []\n",
    "idx = 0\n",
    "for fn, url in csv_files.items():\n",
    "    print(f'Index {idx}: {fn}\\n{url}')\n",
    "    csv_index.append(fn)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37b6b2",
   "metadata": {},
   "source": [
    "### Select and load the sample metadata csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d45ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file_idx = 0\n",
    "\n",
    "# ===================================\n",
    "# get file_url\n",
    "fn = csv_index[metadata_file_idx]\n",
    "fn_url = csv_files.get(fn)\n",
    "\n",
    "if not fn_url:\n",
    "    print('Something is amiss! Could not find file_url. Try again.')\n",
    "else:\n",
    "    try:\n",
    "        headers, metadata_df = make_pandas_df(fn_url, print_headers=False)\n",
    "        print(f'{fn} was loaded as a pandas dataframe.')\n",
    "    except Exception as e:\n",
    "        print(f'Error while attempted to read the {fn_url} into a pandas dataframe. Try again.\\nError: {e}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d2726",
   "metadata": {},
   "source": [
    "=========================================================================\n",
    "\n",
    "## Zip File Option: Inspect zipped dataset contents\n",
    "\n",
    "Otherwise if csv sample metadata files were found, skip down to the end of the Zip File section.\n",
    "\n",
    "### Inspect all dataset files if sample_metadata.csv is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if sample_metadata csv file is not found\n",
    "\n",
    "# ===================================\n",
    "inspect_dataset_distribution(dataset_detail, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b8586",
   "metadata": {},
   "source": [
    "### Select zip file to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a34a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if sample_metadata is not found at the top-level of the dataset contents.\n",
    "zip_file_idx = 7\n",
    "\n",
    "# ===================================   \n",
    "fn, zip_download = inspect_zip_file_contents(dataset_detail, zip_file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030267c4",
   "metadata": {},
   "source": [
    "### Select csv file within zip file to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e412bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if csv file is zipped up\n",
    "csv_file_idx = 2\n",
    "\n",
    "# If needed adjust the number of rows to skip. The Sample ID and Metadata RF specifies 1 header row.\n",
    "header_rows = 1\n",
    "\n",
    "# ===================================\n",
    "csv_file_name = zip_download.namelist()[csv_file_idx]\n",
    "print(f'Attempting to read: {csv_file_name} from zip file {fn}')\n",
    "\n",
    "metadata_df = read_zipped_csv(zip_download, csv_file_name, header_rows)\n",
    "\n",
    "if metadata_df is not None:\n",
    "    is_csv_zipped = True\n",
    "    headers = list(metadata_df.columns)\n",
    "    display(metadata_df)\n",
    "else:\n",
    "    print('ERROR: Sample metadata file was not successfully loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85f366",
   "metadata": {},
   "source": [
    "### End zip file section\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e92e7",
   "metadata": {},
   "source": [
    "## Review sample metadata\n",
    "\n",
    "### View sample metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20508be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metadata_df is not None:\n",
    "    print(f'Success! {fn} loaded as a pandas dataframe with the following column names:\\n')\n",
    "            \n",
    "    for idx, header in enumerate(headers):\n",
    "        print(f'Index {idx} --- {header}')\n",
    "        if header == 'IGSN':\n",
    "            igsn_col_idx = idx\n",
    "            \n",
    "    if igsn_col_idx is None:\n",
    "        print('\\nRequired column name \"IGSN\" was not found. The following code may not work.')\n",
    "    else:\n",
    "        print(f'\\nRequired \"IGSN\" column {igsn_col_idx} was detected.')\n",
    "else:\n",
    "    print('Valid dataframe was not create. Please try again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7a659",
   "metadata": {},
   "source": [
    "### Select metadata columns to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98945f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter column indices from above\n",
    "metadata_columns_idxs = [igsn_col_idx, 3, 19, 21, 30, 33, 32, 1, 5, 6]\n",
    "\n",
    "# ===================================\n",
    "\n",
    "display(metadata_df.iloc[:, metadata_columns_idxs])\n",
    "print('==============================')\n",
    "for col_idx in metadata_columns_idxs:\n",
    "    print(f'Index {col_idx} --- {headers[col_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f67ce",
   "metadata": {},
   "source": [
    "### Inspect unique values in a specified column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_column_idx = 0\n",
    "\n",
    "# ===================================\n",
    "\n",
    "metadata_col = headers[metadata_column_idx]\n",
    "\n",
    "unique_df = metadata_df.iloc[:, [igsn_col_idx, metadata_column_idx]].groupby(metadata_col)\n",
    "\n",
    "display(unique_df.count())\n",
    "unique_values = list(unique_df.groups.keys())\n",
    "\n",
    "print('========================================')\n",
    "print(f'Unique values of metadata colum {metadata_col}:')\n",
    "for idx, val in enumerate(unique_values):\n",
    "    print(f'Index {idx} -- {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca559463",
   "metadata": {},
   "source": [
    "### Select a subset of the metadata based on a unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter values of interest\n",
    "value_idxs = [2]\n",
    "\n",
    "# ===================================\n",
    "subset_values = [x for idx, x in enumerate(unique_values) if idx in value_idxs]\n",
    "\n",
    "subset_df = metadata_df[metadata_df[metadata_col].isin(subset_values)]\n",
    "\n",
    "display(subset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cdc63",
   "metadata": {},
   "source": [
    "### Download sample_metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "\n",
    "if not is_csv_zipped:\n",
    "    fn = csv_index[metadata_file_idx]\n",
    "    all_file_idx = None\n",
    "\n",
    "    for idx, filename in enumerate(dataset_detail.get('distribution')):\n",
    "        if filename.get('name') == fn:\n",
    "            all_file_idx = idx\n",
    "            break\n",
    "    if all_file_idx:\n",
    "        ds_doi = download_selected_files(dataset_detail, [all_file_idx], local_dir)\n",
    "    else:\n",
    "        print('Could not find requested file.')\n",
    "else:\n",
    "    ds_doi = download_selected_files(dataset_detail, [zip_file_idx], local_dir, is_csv_zipped=is_csv_zipped, \n",
    "                                     zip_download=zip_download, zip_member_fn=csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e986b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: display the download file log for this DOI\n",
    "# ===================================\n",
    "print(f'Downloaded file information for {ds_doi}:')\n",
    "display(download_file_log[ds_doi])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f5fab",
   "metadata": {},
   "source": [
    "# 7. Save Download File Log\n",
    "\n",
    "If desired, change save location and file location.\n",
    "Otherwise the local_dir configured at the begining of the notebook will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172064b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: display the download file log\n",
    "display(download_file_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: change the directory location to save the file\n",
    "\n",
    "save_dir = local_dir  # Path('<enter_alternative_dir_path_here')\n",
    "log_filename = 'essdive_downloaded_files_log.csv'\n",
    "\n",
    "# ===================================\n",
    "\n",
    "log_fn_path = save_dir / log_filename\n",
    "\n",
    "with open(log_fn_path, mode='w') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(['dataset_id', 'file_name', 'access_datetime', 'access_url', 'dataset_name', 'citation'])\n",
    "    \n",
    "    for ds_id, log_info in download_file_log.items():\n",
    "        ds_name = log_info.get('name')\n",
    "        ds_citation_ls = log_info.get('citation')\n",
    "        \n",
    "        # deal with the list of citations\n",
    "        ds_citation = None\n",
    "        if ds_citation_ls:\n",
    "            for iref, ref in enumerate(ds_citation_ls):\n",
    "                if iref == 0:\n",
    "                    ds_citation = ref\n",
    "                    continue\n",
    "                ds_citation = f'{ds_citation} -AND- {ref}'\n",
    "        if ds_citation is None:\n",
    "            ds_citation = 'None'\n",
    "        \n",
    "        accessed_file_list = log_info.get('downloaded_files')\n",
    "        for accessed_file in accessed_file_list:\n",
    "            fn, fn_url, access_ts = accessed_file\n",
    "            \n",
    "            csv_writer.writerow([ds_id, fn, access_ts, fn_url, ds_name, ds_citation])\n",
    "            \n",
    "print(f'Check {str(save_dir)} for the log file: {log_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e9b0a",
   "metadata": {},
   "source": [
    "That's a wrap!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
